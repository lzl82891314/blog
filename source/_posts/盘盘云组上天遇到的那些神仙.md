---
title: 盘盘云组上天遇到的那些神仙
url_name: Explore-the-past-and-present-of-Docker-and-Kubernetes
date: 2020-12-31 20:10:58
categories:
  - Docker
  - Kubernetes
---

在我们组上天的过程中，碰到了很多之前开发中完全用不到的技术知识点，也就是我所说的“神仙”，今天我就来给大家盘一盘这些个“神仙”到底都是些什么玩意。

<!-- more -->

## 继续从 PaaS 说起

### 容器的兴起

早在 2013 年前后，服务端的技术是云端计算的天下，当时有大量的云计算厂商涌入市场，比较著名的有`AWS`和`OpenStack`，他们将虚无缥缈的云端计算这种高大上的概念，固化成了实实在在的虚拟机技术，当时的主流用户，也都是将自己的应用上传到云端的虚拟机中，以之前管理物理机的方式管理着可能是一群云端的虚拟服务器。

而就在这个时候，一家主打开源 PaaS 项目的平台`Cloud Foundry`诞生了。与传统的提供云上虚拟机服务不同，Cloud Foundry 的 PaaS 所提供的服务是`应用托管`的功能，也就是用户只需要通过一条简单的命令比如：`cf push "我的应用"`，就可以把自己的项目部署在可能成千上万台的终端服务器上。然后平台对这些项目提供分发，灾备，监控，重启等等服务（这其实也是我们组想要给用户提供的最终服务），这些托管服务解放了开发者的生产力，让他们不用再关心应用的运维状况，而是专心开发自己的应用。这就是 PaaS 的概念，平台即服务。

因此，像 Cloud Foundry 这样的 PaaS 项目，最核心的组件就是对一套应用的打包和分发机制，Cloud Foundry 为每一种主流的语言都定义了一套打包的方式。而 cf push 命令，就是将应用打好的包上传到云端的存储中，然后 Cloud Foundry 调度出一个可运行的虚拟机，通知该虚拟机上的 Agent 下载并且运行这个打包结果。由此可见，Cloud Foundry 的虚拟机中会运行多个用户的多个服务，为了解决服务之间相互隔离的边界问题，Cloud Foundry 使用了 Linux 的 Namespace 技术对运行的应用进行了隔离和分组，使用 Cgroups 技术对隔离的应用分配资源，而这也正是容器技术的一部分。

由此我们可以知道，容器技术并不是 Docker 创建的，而且在 Docker 兴起之前，就已经被其他公司商用了，但是为什么现在一谈起容器，所有人第一时间想到的就是 Docker 呢？

上面，我提到了 Cloud Foundry 的部署需要用户对其应用进行打包，然而就是这个打包的功能，成了 Cloud Foundry 的一个软肋一直被用户诟病：为了上传 PaaS，用户就不得不为每一种语言，每一种框架，甚至是每个版本应用维护一个打好的包，这种打包的方法是毫无章法的，还有可能出现本机运行成功，打了个包上传上去之后就无法运行的情况。然而就在这个时候，一个开源项目在社区的热度开始逐渐升温了，这个项目就是第一节的主人公`Docker`。

Docker 是一个当时还叫 dotCloud 的公司开发的容器项目，在开源的短短几个月后就迅速崛起，并一举将 Cloud Foundry 赶出了局，然而最可笑的是，在 Docker 刚开源的时候，Cloud Foundry 的首席产品经理 James Bayer 就在社区做了一次详细的对比，告诉用户 Docker 和 Cloud Foundry 一样是一个使用了 Namespace 和 Cgroups 技术的沙箱而已，没什么值得关注的。事实上，Docker 也确实就和他所说的一样，但是只做了一点小小的创新，而就是这一点小小的创新，对 Cloud Foundry 造成了毁灭性的打击，这个创新就是`Docker镜像`。

Docker 镜像几乎完美地解决了 Cloud Foundry 对于打包方面的软肋。所谓的镜像，其实也是一个压缩包，但是比起 Cloud Foundry 那种执行文件+启动脚本的打包结果，镜像提供给用户的是一套完整的运行环境，每一个镜像都可以指定操作系统版本，内部可以构建程序执行的文件结构，并且一份镜像可以完全共享多处使用。Docker 给用户提供了一套完善的镜像制作流程，用户也就不用关心自己的语言和框架了，只需要定制对应程序所需要的运行的操作系统环境即可（这一点其实也可以简单的看成是对压缩包这种应用做了底层的抽象，比较契合软件设计的原则。也可以看到，软件行业的一点创新，带来的就是降维级别的打击）。总结一下就是：`Docker 镜像完美解决了两个问题：1是本地环境和服务器环境的差异问题；2是一份镜像所有的机器都可以运行的复用问题`。

在这之后，PaaS 的市场，已经完全是 Docker 的天下了。而之后的故事，我会在下一节再继续讲。

### 一个进程的诞生

好了，讲了半天的故事了，也要说一点干货了：容器到底是个什么玩意，能有这么大的市场和这么多服务端程序员对其青睐？先别着急，我先从一个进程说起。

我们都知道，计算机里运行的程序其实都是一个一个的进程，而一个进程其实就是程序执行之后，从磁盘的二进制文件，到内存、寄存器、堆栈指令等等所用到的相关设备状态的一个集合，是`数据和状态综合的动态表现`。而容器技术，其实就是对一个进程的状态和数据进行的一系列`隔离`和`限制`后的结果。因此可以知道，`容器的本质其实就是Linux中的一个特殊进程`。

先来说说隔离，我们可以在任意一个装有 Docker 程序的 Linux 中执行下列的命令创建一个简单的镜像：

```shell
$ docker run -it busybox /bin/sh
```

这条语句的大概意思是：用 docker 运行一个容器，容器的镜像名称叫`busybox`，并且运行之后需要执行的命令是`/bin/sh`，而`-it`参数表示需要使用标准输入`stdin`和分配一个文本输入输出环境`tty`与外部交互。通过这个命令，我们就可以进入到一个容器内部了，分别在容器中和宿主机中执行`top`命令，可以看到以下结果：

![host_container_comparer](https://image.dunbreak.cn/cloud/host_container_comparer.png)

可以发现，容器中的运行进程只剩下了两个，一个就是主进程 PID==1 的/bin/sh，另一个就是我们运行的 top，而宿主机中的其余的所有进程在容器中都看不到了，这就是`隔离`。本来，每当我们在宿主机上运行一个/bin/sh 程序，操作系统都会给它分配一个进程编号，比如 PID==100。而现在，我们要通过 Docker 把这个/bin/sh 程序运行在一个容器中，这时候，Docker 就会在这个 PID==100 创建时施加一个“障眼法”，让他永远看不到之前的 99 个进程，这样运行在容器中的程序就会当自己是 PID==1 的主进程。

> 这里说明一下，在 Linux 操作系统中，PID==1 的进程被称为超级进程，它是整个进程树的 root，负责产生其他所有用户进程。所有的进程都会被挂在这个进程下，如果这个进程退出了，那么所有的进程都被 kill

而这种机制，其实就是对被隔离的程序的进程空间做了手脚，虽然在容器中显示的 PID==1，但是在原本的宿主机中，它其实还是那个 PID==100 的进程。所使用到的技术就是 Linux 中的`Namespace机制`。而这个机制，其实就是 Linux 在创建进程时的一个可选参数。在 Linux 中，创建一个线程的函数是（这里没写错就是线程，Linux 中线程是用进程实现的，所以可以用来描述进程）：

```shell
int pid = clone(main_function, stack_size, SIGCHLD, NULL);
```

如果我们给这个方法添加一个参数比如`CLONE_NEWPID`：

```shell
int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL);
```

那么这个新的进程就会看到一个全新的进程空间，在这个空间里，它自己的 PID 就等于 1。

`除了刚才说的PID Namespace，Linux操作系统还提供了Mount、UTS、IPC、Network和User的Namespace，用来对各种不同的进程上下文进行隔离操作`。

以上，也就是 Linux 容器最基本的隔离的实现了。

说完了隔离，再来说说限制。容器的隔离性是通过 Linux 的`Cgroups`实现的。`Cgroups`的全称是`Linux Control Group`，是 Linux 操作系统中用来`限制一个进程组使用资源的上限，包括CPU、内存、磁盘、网络带宽等`的功能。在 Linux 中，Cgroups 给用户暴露的 API 是文件系统，因此用户可以通过修改文件的值来操作 Cgroups 功能。在 Linux 系统（Ubuntu）中可以执行以下命令查看 CgroupsAPI 文件：

```shell
mount -t Cgroups
```

![cgroups_file](https://image.dunbreak.cn/cloud/cgroups_file.png)

从上图可以看到，系统中存在包括 cpu、内存、IO 等多个 Cgroups 配置文件。我们可以以 CPU 为例来说明以下 Cgroups 这个功能。对 CPU 的限制需要引入两个参数`cfs_period`和`cfs_quota`，这个强哥肯定知道当时为了给 Docker 内的程序限制 CPU 会经常操作这两个参数，这两个参数是组合使用的，意思是在长度为 cfs_period 时间内，程序组只能分到总量为 cfs_quota 的 CPU 时间。也就是说`cfs_quota / cfs_period == cpu使用上限`。

举个栗子，在`/sys/fs/Cgroups/cpu`目录下，执行以下命令创建一个文件夹 container：

```shell
/sys/fs/Cgroups/cpu/ > mkdir container
```

可以发现系统会自动为 container 目录下生成一系列的 CPU 限制的参数文件，这是 Linux 系统自动生成的，表示我们成功为 CPU 创建了一个控制组 container：

![cpu_limit_file](https://image.dunbreak.cn/cloud/cpu_limit_file.png)

然后我们可以执行一下以下脚本创建一个死循环程序：

```shell
while : ; do : ; done &
```

会看到返回的`进程为398`，因为死循环所以 top 可以看到 cpu 占用率为 100%：

![cpu_full](https://image.dunbreak.cn/cloud/cpu_full.png)

这时，我们可以看下 container 目录下的`cpu.cfs_quota_us`和`cpu.cfs_period_us`：

![cpu_usage_file](https://image.dunbreak.cn/cloud/cpu_usage_file.png)

`cfs_quota_us`为-1 说明并没有限制 CPU 的运行上限，现在我们可以改一下这个值：

```shell
echo 20000 > /sys/fs/Cgroups/cpu/container/cpu.cfs_quota_us
```

然后将之前的`进程398`写入这个控制组的`tasks`文件中：

```shell
echo 398 > /sys/fs/Cgroups/cpu/container/tasks
```

这时，我们可以再 top 一下，发现刚才的死循环的 CPU 使用率只有 20%了：

![cpu_usage_after](https://image.dunbreak.cn/cloud/cpu_usage_after.png)

以上，就是通过 Cgroups 功能对容器做限制的原理了，同理可以用此方法，对一个容器的内存、带宽等做限制，这样，一个简单的容器基本就可以展现在你面前了，Cloud Foundry 为自己的 PaaS 平台为每一个应用所创建的就是这样的一个容器。

> 这里要做一个特别的说明，只有 Linux 中运行的容器是通过对进程进行限制模拟出来的结果，Windows 和 Mac 下的容器，都是通过 Docker Desktop 操作虚拟机模拟出来的真实的虚拟容器。

### Docker 做了些什么？

从第一节我们知道，Docker 通过自己的`Docker 镜像`功能迅速取代了 Cloud Foundry，那这个 Docker 镜像到底是什么呢？先卖个关子，我们先来看看之前说过的隔离功能中的另一个机制`Namespace机制`。

`Mount Namespace`从名字你们也知道这个是和文件挂载相关的，是用来隔离进程的挂载目录的，我们可以通过一个“简单”的栗子来看看它是怎么工作的：

```C
#define _GNU_SOURCE
#include <sys/mount.h>
#include <sys/types.h>
#include <sys/wait.h>
#include <stdio.h>
#include <sched.h>
#include <signal.h>
#include <unistd.h>
#define STACK_SIZE (1024 * 1024)
static char container_stack[STACK_SIZE];
char* const container_args[] = {
    "/bin/bash",
    NULL
};

int container_main(void* arg)
{
    printf("Container - inside the container!\n");
    execv(container_args[0], container_args);
    printf("Something's wrong!\n");
    return 1;
}

int main()
{
    printf("Parent - start a container!\n");
    int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL);
    waitpid(container_pid, NULL, 0);
    printf("Parent - container stopped!\n");
    return 0;
}
```

上面是一个“简单的”的 C 语言代码，如果刨去现在已经不记得怎么写 C 语言了之外，这个代码其实真的挺简单的，只有两个逻辑：1 是在 main 函数中创建了一个子进程，并且传递了一个参数`CLONE_NEWNS`，这个参数就是用来实现`Mount Namespace`的；2 是在子进程中调用了`/bin/bash`命令运行了一个子进程内部的 shell。让我们编译并且执行一下这个程序：

```shell
gcc -o ns ns.c
./ns
```

这样我们就进入了这个子进程的 shell 中，我们可以运行`ls /tmp`和宿主机进行一下对比：

![mount_namespace_show_demo](https://image.dunbreak.cn/cloud/mount_namespace_show_demo.png)

你会发现为啥两边展示的数据会完全一样？因为按照上一部分 Cpu Namespace 的结论，应该分别看到两个不同的文件目录才对。这是因为`Mount Namespace`修改的，是进程对文件系统“挂载点”的认知，意思也就是只有发生了`挂载`这个操作之后生成的所有目录才会是一个新的系统，而如果不做挂载操作，那就和宿主机的完全一致。那如何解决这个问题呢？其实就是在创建进程时，除了声明一个`Mount Namespace`之外还告诉这个进程需要进行一次挂载操作就可以了，可以简单修改一下新进程的代码，然后运行查看：

```C
int container_main(void* arg)
{
    printf("Container - inside the container!\n");
    mount("none", "/tmp", "tmpfs", 0, "");
    execv(container_args[0], container_args);
    printf("Something's wrong!\n");
    return 1;
}
```

![mount_namespace_show_demo_new](https://image.dunbreak.cn/cloud/mount_namespace_show_demo_new.png)

现在就会发现，对文件的隔离已经生效了，并且在子进程内部新创建的目录宿主机中一样看不到了，因为子进程的`/tmp`已经被挂载进了`tmpfs（一个内存盘）`中了，这就相当于创建了完全一个新的`tmp`环境。

为什么要花这么大的篇幅来说这个呢，是因为它就`是Docker镜像的主要技术之一`。上面的栗子，我只是把`tmp`这个目录进行了挂载，同样的，在 Cloud Foundry 中挂载隔离的文件也都是应用所需要的一些目录，而 Docker 镜像做的，是把`整个根目录`全部挂载进了这个新的进程中。

> 这里要稍微介绍一下 Linux 系统地文件结构的知识：和我们平时使用的 Windows 操作系统不同，Linux 操作系统的系统**内核**与**操作系统所包含的文件、配置和目录**是`分开存放的`，系统只有在开机启动时才会加载指定版本的内核镜像。Linux 所使用到的文件配置目录被称为`根文件系统（rootfs）`，也就是我们日常操作 Linux 看到的`/`目录。

Docker 镜像的本质，其实也就是对 rootfs 的一次封装，Docker 将一个应用所需操作系统的 rootfs，通过`Mount Namespace`进行了封装，这样带来的直接结果就是：**改变了应用程序和操作系统的依赖关系**（这个很类似于强哥上一次讲到的控制反转的理念）即原本应用程序是在操作系统内运行的，而 Docker 把“操作系统”封装变成了应用程序的依赖库，这样就解决了应用程序运行环境`一致性`的问题，因为不管你是在自己主机上，还是服务器上，又或者是其他什么机器上，因为应用所运行的系统已经成了一个“依赖库”了，所以每个地方运行，这个一致性是完全可以保证的。

但是目前只解决了一致性，复用性的问题还没有解决，即我不可能每制作一个镜像就挂载一个新的 rootfs 吧，这样先不说复用性了，光硬盘都需要占用很大的空间来实现这些挂载内容。因此，Docker 还为镜像使用了`另一个主要技术：UnionFS`以及一个`全新的概念：层（layer）`。

先说一下`UnionFS`是干什么的，UnionFS 是一个联合挂载的功能，它可以将多个路径下的文件联合挂载到同一个目录下，举个栗子，现在有一个如下的目录结构：

```shell
$ tree
.
├── A
│   ├── a
│   └── x
│
└── B
    ├── b
    └── x
```

A 目录下有 a 和 x 两个文件，B 目录下有 b 和 x 两个文件，通过 UnionFS 的功能，我们可以将这两个目录挂载到 C 目录下：

```shell
$ tree ./C
C
├── a
├── b
└── x
```

最终 C 目录下的 x 只有一份，并且如果你对 C 目录下的 a、b、x 修改，之前目录 A 和 B 中的文件同样会被修改。而 Docker 正是用了这个技术，对其镜像进行了联合挂载，比如可以分别把`/sys`，`/etc`,`/tmp`目录一起挂载到 rootfs 中形成一个在子进程看起来就是一个完整的 rootfs。

此外，Docker 还自己创新了一个层的概念，它将系统内核所需要的 rootfs 内的文件挂载到了一个`只读层`中，将用户的应用程序、系统的配置文件等之类可以修改的文件挂载到了`可读写层`中，并且在容器启动时可能会用到的初始化参数挂载到了`init层`中。之后，将这三层再次联合挂载，最终形成了容器中的 rootfs。这时你应该明白了，这个`只读层`是几乎同样版本的代码几乎不会改变的，比如我们的.net core 项目用到的基础环境等等都会设计在了只读层，每次获取最新镜像时，因为每一份只读层都是完全一样的，所以完全不用下载，这也解释了为什么 Docker 镜像只在第一次下载时那么慢，而之后的镜像都很快，并且明明每份镜像看起来都几百兆，但是最终机器上的硬盘缺没有占用那么多的原因。也是这个技术，解决了镜像复用性的问题。我们可以看一下 Docker 镜像分层的图：

![docker_layer](https://image.dunbreak.cn/cloud/docker_layer.png)

这里有个问题不知道你有没有考虑过，如果我们的程序做的是删除只读层中的文件，那会发生什么事呢？其实解决方案是：Docker 在可读写层加入了一个 whiteout 的文件夹（BLM 政治不正确），所以在只读层做的操作其实就在这个文件夹中加入了一个新的文件，然后在容器中就通过这个文件做了`“逻辑删除”`，只是做了`“遮挡”`的操作，原本的文件还是存在的。

好了，以上就是 Docker 容器的整个原理了，我们可以总结一下，Docker 创建容器的过程其实是：

1. 启用 Linux Namespace 配置；
2. 设置指定的 Cgroups 参数；
3. 切换进程的根目录

对于制作一个 Docker 镜像，Docker 使用了联合挂载的功能，和创建了层的概念，解决了镜像的一致性和复用性。

### 对比与思考

其实 Docker 还做了很多功能，比如权限配置，DeviceMapper 等等，这里说的仅仅是一个普及性质的概念性讲解，底层的各种实现还有很复杂的概念。并且或多或少地，你肯定也会有一个想法，这个容器和传统的虚拟机有啥区别？

其实容器技术和虚拟机是实现虚拟化技术的两种手段，只不过虚拟机是通过 Hypervisor 控制硬件，模拟出一个 GustOS 来做虚拟化的，其内部是一个几乎真实的虚拟操作系统，内部外部是完全隔离的。而容器技术是通过 Linux 操作系统的手段，通过类似于 Docker Engine 这样的软件对系统资源进行的一次隔离和分配，它们之间的对比关系大概如下：

![compare_with_v_machine](https://image.dunbreak.cn/cloud/compare_with_v_machine.jpg)

虚拟机相较于 Docker 容器来说更加安全，因为其是物理隔离的，但是这带来一个后果：一个运行着 CentOS 的 KVM 虚拟机启动后，在不做优化的情况下，虚拟机自己就需要占用 100~200 MB 内存。此外，用户应用运行在虚拟机里面，它对宿主机操作系统的调用就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘 I/O 的损耗非常大。相反：容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的；而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。

因此，更加符合现在“敏捷”和“高性能"的容器就成为了 PaaS 这种更需要细粒度资源管理的平台佼佼者。

但是容器的`弊端`也特别明显，因为容器是模拟出来的隔离性，所以对 Namespace 模拟不出来的资源：比如操作系统内核就完全无法隔离，`容器内部的程序和宿主机是共享操作系统内核的，也就是说，一个低版本的Linux宿主机很可能是无法运行高版本容器的`。还有一个典型的栗子就是`时间`，`如果容器中通过某种手段修改了系统时间，那么宿主机的时间一样会改变`。

`还有一个弊端就是安全性`，一般的企业，是不会直接把容器暴露给外部用户直接使用的，因为容器内可以直接操作内核代码，如果黑客可以通过某种手段修改内核程序，那就可以黑掉整个宿主机，这也是为什么我们组从刚开始自己写 Docker 到最后弃用的直接原因。现在一般解决安全性的方法有两个：一个是限制 Docker 内进程的运行权限，控制它值能操作我们想让它操作的系统设备，但是这需要大量的定制化代码，因为你可能并不知道它需要操作什么；另一个方式是在容器外部加一层虚拟机实现的沙箱，这也是阿里云以及美团的主要实现方式，想要具体了解，可以看这篇文档[云原生之容器安全实践](https://tech.meituan.com/2020/03/12/cloud-native-security.html)。

## 大航海时代

### 天下归一

让我们来继续讲故事。

上节说到，Docker 项目依靠自己创新的 Docker Image 瞬间爆红。与此同时，很多厂商也从中发现了商机，纷纷推出自己的容器产品抢占市场，比如 CoreOS 推出了 Rocket（rkt）容器，Google 也开源了自己的容器项目 lmctfy（Let Me Container That For You）等，但是面对 Docker 项目的强势，就算是 Google 也毫无招架之力，因此 Google 打算和 Docker 公司开展合作，关停自己的容器项目，并且和 Docker 公司一同维护开源的容器运行时，但是 Docker 公司很强势的拒绝了这个明显会削弱自己地位的合作。并且 Docker 公司此时也意识到了，自己仅仅是云计算技术栈中的幕后英雄，只能当做平台最终部署应用的载体，要想成为这个领域的核心，就应该有自己的生态。因此，在爆火并且有了充足的资金之后，Docker 公司开始了疯狂的买买买来扩充自己的实力，其中最出名的就署名`Fig`，它是出名的`Docker Compose`项目的前身。通过这些并购与自身研发迭代，Docker 公司最终推出了以自己为核心的 PaaS 三件套：`Docker Compose`、`Docker Swarm`以及`Docker Machine`，并且通过三件套迅速成为了当时行业内的霸主。此外，Docker 公司还做出了一个更加惊人的动作：将公司名由 dotCloud 改名为 Docker，并且将`Docker`注册成了自己的商标，开展自己的商业化路径。但是这样做同时也意味着，其他公司使用 Docker 就要给 Docker 公司支付大额的授权费用，这为 Docker 以后的没落埋下了伏笔。

这里稍微说一下 Docker 三件套：

1. Docker Compose：Compose 的前身是一个仅有两个人维护的 Docker 容器编排的开源项目 Fig，其功能是：假如现在用户需要部署一个应用 A 和一个数据库 B 以及负载均衡 C，那么 Fig 就允许用户把 ABC 三个容器定义在一个配置文件中，并且指定它们的关联关系。最后只需要一条命令`fig up`即可直接使用。在 Docker 大火的时候，Fig 项目在 Github 上的热度堪比 Docker，因此在 2015 年 Docker 公司将其收购，并且改名为 Docker Compose 作为 Docker 容器的编排工具，并且使用至今
2. Docker Swarm：Swarm 是 Docker 的集群管理项目，其主要的逻辑就和上一节讲过的 Cloud Foundry 类似，可以以类似于`docker run 我的镜像`的命令行方式，以`docker run -H 我的Swarm集群API地址 我的镜像`这样将 Docker 运行成一个集群并且进行管理，Swarm 的出现将 Docker 项目从一个普通的容器升级成为 PaaS 平台
3. Docker Machine：Machine 应该是 Docker 三件套里最不出名的了，我对此也没有过多了解，只知道它的功能是将虚拟机运行成容器，并且可以以管理 Docker 容器的方式管理虚拟机

然而人红是非多，更何况你还抢了别人的蛋糕，由于 Docker 公司在 Docker 开源项目的发展上始终保持着绝对的权威和发言权，并且在多个场合用实际行动挑战到了其他玩家，并且，Docker 公司的商业化路径严重侵害了曾今的合作公司 RedHat 的切身利益，再加之，Docker 在 2015 年间的高速迭代表中现出了各种不稳定的 breaking change 开始让社区叫苦不迭。于是，容器领域的其他几位玩家开始商讨“切割”Docker 项目的话语权。最终，Docker 公司迫于压力，于 2015 年 6 月 22 日，牵头了 CoreOS、Google、RedHat 等公司，共同成立了一个中立的基金会，并将自己的容器运行时 LibContainer 捐出，改名为 RunC，基金会依据 RunC 共同制定了一套容器和镜像的标准和规范——OCI（Open Container Initiative），OCI 的成立，意味着容器运行时和镜像的实现与 Docker 项目完全剥离，让其他玩家不依赖 Docker 实现自己的 Docker 运行时成为可能。

但是，由于成立基金会只是 Docker 公司对这些大公司的妥协，并且其当时确实维护着数量足够庞大的社区，因此 Docker 公司对基金会的成立有恃无恐，并且对标准的制定并不关心。因为在当时的大环境下，Docker 自己的实现，已经就是业界统一的标准了。

由于 OCI 基金会发展十分缓慢，因此 Google、RedHat 等基础设施领域的头部玩家打出了手中的王牌，共同牵头成了一个名叫`CNCF（Cloud Native Computing Foundation）的基金会`（这也是云原生这个概念第一次出现在大众视野内），并且基金会成立的思想基础是，以`Kubernetes项目`为基础，建立一个由开源基础设施领域厂商主导的按照独立基金会方式运营的平台级社区，来对抗以 Docker 为核心的容器商业生态。也正是这个基金会的成立，我们第二节的主人公`Kubernetes`也开始崭露头角了。

`Kubernetes`是 Google 公司早在 2014 年就发布开源的一个`容器基础设施编排`框架，和其他拍脑袋想出来的技术不同，`Kubernetes`的技术是有理论依据的，即：`Borg`。[Borg](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43438.pdf)是 Google 公司整个基础设施体系中的一部分，Borg 是 Google 公司整个基础设施体系中的一部分，Google 也发布了多篇关于 Borg 的论文作为其理论支持。其上承载了比如 MapReduce、BigTable 等诸多业界的头部技术。因此 Borg 系统一直以来都被誉为 Google 公司内部最强大的“秘密武器”，也是 Google 公司最不可能开源的项目，而`Kubernetes`就是在这样的理论基础上开发的。下图是 Google Omega 论文所描述的 Google 已公开的基础设施栈。

![borg_infr](https://image.dunbreak.cn/cloud/borg_infr.png)

但是，也正由于开发灵感和设计思想来源于 Borg，`Kubernetes`项目在刚发布时就被称为`曲高和寡`，太过超前的思想让开发者无法理解，并且也一直由 Google 的工程师自行维护，所以在发布之初并没有获得太多的关注和成长。

然而，CNCF 的成立改变了这一切，RedHat 的长处就是有着成熟的社区管理体系，并且也有足够多的工程能力，这使得`Kubernetes`项目在交由社区维护开始迅速发展，并且逐渐开始和 Docker 分庭抗礼。并且和 Docker 的封闭商业模式不同，Kubernetes 反其道而行之主打开源和民主化，每一个重要功能都给用户提供了可定制化的接口，并且普通用户也可以无权限拉取修改 Kubernetes 的代码，社区有专门的`reviewer`以及`approver`，只要你写的代码通过 PR 通过了代码审核和批准，就会成为 Kubernetes 的主干代码，这也大大的增加了 Kubernetes 的活力。并且，依托于开放性接口，基于 Kubernetes 的开源项目和插件比比皆是，并且依托于优秀的架构设计，微服务等新兴的技术理念也迅速落地，最终形成了一个百花齐放的稳定庞大的生态。

面对 Kubernetes 社区的崛起和壮大，Docker 公司不得不承认自己的豪赌以失败告终，2017 年开始，Docker 将 Docker 项目的容器运行时部分 Containerd 捐赠给了 CNCF 社区，并且在 10 月宣布将在自己的 Docker 企业版中内置 Kubernetes 项目，这也标志着持续了近两年的容器编排之战落下帷幕。2018 年 1 月，RedHat 公司宣布斥资 2.5 亿美元收购 CoreOS，2018 年 3 月，这一切纷争的始作俑者 Docker 公司的 CTO Solomon Hykes 宣布辞职，至此，纷扰的容器技术圈尘埃落定，天下归一。

### Kubernetes 的创新

上文虽然提到了 CNCF 社区的活力对比 Docker 生态的封闭获得了大量的成功，但光社区活力并不足以让 Docker 公司这么迅速的败下阵来，Kubernetes 击败 Docker 的主要原因还是对`容器编排`这项技术的超前理解对 Docker 公司来说基本就是**降维打击**，毫无还手之力。

从上文我们知道，Docker 所构建的，是以 Docker 容器为最核心的 PaaS 生态，包括以`Docker Compose`为主的简单容器关系编排，以及以`Docker Swarm`为主的线上运维平台，用户可以通过 Docker Compose 处理自己集群中容器之间的关系，并且通过 Docker Swarm 管理运维自己的集群，可以看到这一切其实就是当初 Cloud Foundry 的 PaaS 功能，所主打的就是和 Docker 容器的无缝集成。因此，Kubernetes 如果要和 Docker 对抗，肯定不能再做 Docker 容器管理这种 Docker 本身就已经支持的功能了，这样的话别说分庭抗礼，可能连 Docker 的基本用户都吸引不到。因此在设计之初，就确定了**不以 Docker 为核心依赖**的设计理念，所以在 Kubernetes 中，Docker 仅是容器运行时实现的一个可选项，用户可以依据自己的喜好任意调换自己想用并且支持了 Kubernetes 提供接口的任意容器。此外，Kubernetes 准确的抓住了 Docker 容器的一个致命性的弱点进行了自己的创新。

这个弱点是什么我们先不谈，让我们先来看看什么是`容器编排`。所谓容器编排，其实就是处理容器和容器之间的关系，因为一个分布式的大型系统里，不可能是以多个单一个体存在的，它们可能是一个与多个，一群与一群这样相互交织着。Docker Compose 做到的是为多个有交互关系建立一种“连接”，把它们全部编写在一个`docker-compose.yaml`文件中，然后统一发布，我后面说到的组里的 ELK 功能就是这样做的，这样做也有优点，就是对于简单的几个容器之间的交互来说非常便利。但是对于大型的集群来说真的有点杯水车薪了，并且这种每出现一种新需求就单独做一项新功能的开发模式，在后期代码会十分难以维护。

而与 Docker 这种站在容器视角上只能处理容器之间的关系所不同，Kubernetes 所做的是以软件工程的设计理念将关系进行了不同之类的划分，定义了`紧密关系（Pod之间）`和`交互关系（Service之间）`的概念，然后再对不同的关系进行特定的编排实现。乍一听可能是一头雾水，我举一个不太实际但是一看就懂的栗子：如果把容器之间的关系比作人之间的关系，Docker 能处理的是`站在单一个体的角度上`处理人与人之间的人际关系，而 Kubernetes 就是上帝，`站在上帝视角`不仅能处理人与人之间的人际关系，还能处理狗与狗之间的狗际关系，最主要的是能处理人与狗之间的交往关系。

而实现上述`紧密关系`的原理，就是 Kubernetes 的创新**Pod**了。`Pod是Kubernetes所创新的一个概念，其原型是Borg中的Alloc，是Kubernetes运行应用的最小执行单元，由一个或者多个紧密协作的容器组合而成`，其出现的原因是针对容器的一个`致命性弱点——单一进程`这问题的扩展，让容器有了进程组的概念。通过第一节，我们知道了容器的本质是一个进程，其本身就是超级进程，其他进程都必须是它的子进程，因此在容器中，没有进程组的概念，而在日常的程序运行中，进程组是常常配合使用的，如：

> Linux 中有一个负责操作系统日志处理的程序 rsyslogd 是由三个模块：imklog 模块、muxsock 模块以及 rsyslogd 自己的 main 函数主进程组成。这三个进程组一定要运行在同一台机器上，否则它们之间的基于 Socket 的通信和文件的交换都会出现问题。

而上述的这个问题，如果出现在 Docker 中，就不得不使用三个不同的容器分别描述了，并且用户还得自己模拟处理它们三个之间的通信关系，这种复杂度可能比使用容器运维都高的多。并且对于这个问题的运维，Docker Swarm 也有自己本身的问题。以上述的栗子为基础，如果三个模块分别都需要 1GB 的内存运行，如果 Docker Swarm 运行的集群中有两个 node，node-1 剩余 2.5GB，node-2 剩余 3GB。这种情况下分别使用`docker run 模块`运行上述三个容器，基于 Swarm 的`affinity=main约束`，他们三个都必须要调度到同一台机器上，但是 Swarm 却很有可能先分配两个去 node-1，然后剩余的一个由于还剩 0.5GB 不满足调度而使这次调度失败。这种典型的成组调度（gang scheduling）没有被妥善处理的栗子在 Docker Swarm 中经常存在。

基于上述的需求，Kubernetes 有了`Pod`这个概念来处理这种紧密关系。在一个 Pod 中的容器共享相同的 Cgroups 和 Namespace，因此它们之间并不存在边界和隔离环境，它们可以共享同一个网络 IP，使用相同的 Volume 处理数据等等，其原理其实很简单，就是在多个容器之间创建其共享资源的链接。但是为了解决到底是 A 共享 B，还是 B 共享 A，以及 A 和 B 谁先启动这种拓扑性的问题，一个 Pod 其实是由一个 Infra 容器联合 AB 两个容器共同组成的，其中 Infra 容器是第一个启动的：

![pod](https://image.dunbreak.cn/cloud/pod.png)

Infra 容器是一个用汇编语言编写的、主进程是一个永远处于“暂停”状态的容器，其仅占用极少的资源，解压之后也仅有 100KB 左右。

现在，让我们看看在 Kubernetes 中，这样的 Pod 长什么样吧。我们在任意一个装有 Kubernetes 的集群中通过以下的 yaml 文件和 shell 命令运行处一个 Pod，这个 yaml 文件具体是什么意思暂时不用理会，之后我会对这个 yaml 做一说明，我们目前只需要明白：`Kubernetes中的所有资源都可以通过以下这种yaml文件或者json文件描述的`，现在我们只需要知道这是一个运行着 busybox 和 nginx 的 Pod 即可：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hello-pod
spec:
  shareProcessNamespace: true # 加入了此参数，容器内才会共享ProcessNamespace
  containers:
    - name: hello-busybox
      image: busybox
      stdin: true
      tty: true
    - name: hello-nginx
      image: nginx
```

创建这个 hello-pod.yaml 文件之后运行以下命令：

```shell
# 创建Pod
kubectl apply -f hello-pod.yaml

# 成功后进入执行此命令进入此Pod的busybox shell
kubectl attach -it hello-pod -c hello-busybox

# 在busybox shell中执行
ps ax
```

通过上述命令，我们就成功创建了一个 pod，我们可以从执行结果看到 infra 容器的主进程成为了此 Pod 的 PID==1 的超级进程，说明了 Pod 是组合而成的：

![infra_ps](https://image.dunbreak.cn/cloud/infra_ps.png)

至此，我们应该要理解 Pod 是 Kubernetes 的最小调度单位这个概念了，并且也应该把 Pod 作为一个整体而不是多个容器的集合来看待。

现在，我们回过头来看看描述了这个 Pod 的文件类型 YAML 吧。

首先来看看 YAML 的语法定义：

> YAML 是一种专门编写配置文件的语言，其简洁且强大，在描述配置文件方面远胜于 JSON，因此在很多新兴的项目比如 Kubernetes 和 Docker Compose 等都通过 YAML 来作为配置文件的描述语言。与 HTML 相同，YAML 也是一个英文的缩写：YAML Ain't Markup Language，聪明的同学已经看出来了，这是一个递归写法，突出了满满的程序员气息。其语法有如下特征：

- 大小写敏感
- 使用缩进表示层级关系，类似 Python
- 缩进不允许使用 Tab，只允许使用空格
- 缩进的空格数目不重要，只要相同层级的元素左侧对其即可
- 数组用短横线`-`表示
- NULL 用波浪线`~`表示

有了以上概念，我们就可以把上述的 YAML 改写成一个 JSON，来看看它们的区别：

```json
{
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": {
    "name": "hello-pod"
  },
  "spec": {
    "shareProcessNamespace": true,
    "containers": [
      {
        "name": "hello-busybox",
        "image": "busybox",
        "stdin": true,
        "tty": true
      },
      {
        "name": "hello-nginx",
        "image": "nginx"
      }
    ]
  }
}
```

这两种写法在 Kubernetes 中是等效的，上述的 JSON 也可以正常运行，但是 Kubernetes 还是更推荐使用 YAML，因此从上面的对比中我们也能发现，之前一直感觉很香的 JSON 现在看起来也不香了，因此需要些大量的字符串标志。其上只列出了 YAML 日常开发中用到的常用的功能，具体如果还想继续了解，可以看我最后参考文档中的 YAML 教程。

看完语法，我再来说说上述 YAML 中的各个节点在 Kubernetes 所表示的意思。Kubernetes 中的有一种类似于 Java 语法万物皆对象的概念，所有内部的资源，包括服务器 node、服务 service 以及运行组 Pod 在 kubernetes 中皆是以对象的形式存储的，其所有对象都由一下固定的部分组成：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample
  # ...
spec:
# ...
```

- apiVersion：在官方文档中并没有给出相应的解释，但是从名字可以看出这是一个`规定API版本`的字段，但是此字段不能自定义，必须符合 Kubernetes 的官方约束，目前我们用到的基本都是 v1，表示稳定版
- kind：指明当前的配置是什么类型，比如 Pod、Service、Ingress、Node 等，注意这个首字母是大写的
- metadata：用于描述当前配置的 meta 信息，比如 name，label 等
- spec：是指明当前配置的具体实现

所有的 Kubernetes 对象基本都满足以上的格式，因此最开始 Pod 的 yaml 文件的意思是“使用 v1 稳定版本的 API 信息，类型是 Pod，名称是 hello-pod，具体实现是开启 ProcessNamespace，有两个容器，信息分别为…”。

知道了 YAML 的概念，让我们在回归主题。为了解决容器单一进程只是创建 Pod 的原因之一，还有一个比较重要的原因是 Google 通过 Pod 实现了自己的`容器设计模式`。设计模式大家肯定都知道，一个简单地单例模式连大一的学生都会写，而 Google 则为 Kubernetes 编写了适合其开发的容器设计模式，其[论文](https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_burns.pdf)感兴趣的同学也可以自己去学习。

举个最常用的栗子：

> 我们知道 Java 项目并不能像.Net Core 项目那样编译完成后直接自宿主运行，必须要把编译生成的 war 包拷贝到服务宿主程序比如 Tomcat 的运行目录下才可以正常使用。但是越大的公司分工越明确，很有可能 Java 项目和服务宿主程序不是同一个人甚至同一个部门开发的，因此为了让他们各自独立开发并且还可以紧密合作，我们可以把它们同时写在一个 Pod 中。

下面这个 yaml 文件就定义了一个满足上述需求的 Pod：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-java
spec:
  initContainers:
    - image: sample-java:latest
      name: war
      command: ["cp", "/sample.war", "/app"]
      volumeMounts:
        - mountPath: /app
          name: sample-volume
  containers:
    - image: sample-tomcat:latest
      name: tomcat
      command: ["sh", "-c", "/root/apache-tomcat/bin/start.sh"]
      volumeMounts:
        - mountPath: /root/apache-tomcat/webapps
          name: sample-volume
      # ports:
  # ... ...
  volumes:
    - name: sample-volume
      emptyDir: {}
```

在这个 yaml 文件中，我定义了一个 java 程序和 tomcat 程序的容器，并且对这两个容器之间的容器进行了一次挂载操作：将 java 程序的/app 路径以及 tomcat 程序的/root/apache-tomcat/webapps 同时挂载到了 sample-volume 这个挂在卷上，并且最后定了这个挂载卷就是一个内存数据卷。并且定义了 java 程序所在的容器是一个 initContainer，说明此容器是在 tomcat 容器之前启动的，并且启动之后执行了一个 cp 的命令。

> 综上，我们可以看到上树的 Pod 描述了这样一个场景：程序运行开始运行时，Java 容器启动，把自己的 war 包 sample.war 拷贝到了`自己的/app目录`下；之后 tomcat 容器启动，执行启动脚本，执行的 war 包从`自己的/root/apache-tomcat/webapps`路径下获得。

可以看到通过上述的配置描述，我们既没有改动 Java 程序，也没有改动 tomcat 程序，却让它们完美的配合工作了，完成了解耦操作。这个栗子就是容器设计模式中的`Sidecar模式`，还有很多设计模式，感兴趣的同学可以去[《Kubernetes 与云原生应用》之容器设计模式](http://www.dockerinfo.net/2067.html)学习使用。

以上就是 Kubernetes 为了解决`紧密关系`而抽象出来的概念 Pod 的基础内容了，需要注意的是，Pod 提供的只是一种编排的思想，而不是具体的技术方案，在我们使用的 Kubernetes 框架中，Pod 只不过是以 Docker 作为载体实现了而已，如果你使用的底层容器是虚拟机比如[virtlet](https://github.com/Mirantis/virtlet)，那这个 Pod 创建时就根本不需要那个 Infra Container，因为虚拟机天生就支持多进程协同。

### 容器的编排大戏

然而，光有了 Pod 是不能够解决用户部署应用的需求的，大部分用户用户所关心的是如何调度管理自己的应用，这部分功能才是 Kubernetes 最强大的杀器。比如现在用户的需求是：

> 以 3 机负载均衡的形式部署一个私有云客户的应用，这应该如何实现呢？

这项工作如果在我们之前自己开发的版本中实现起来大概是这样的：需要三个不同的物理机，在这三个物理机上分别安装把用户的应用 run 成容器，然后在三台服务器之外再买一个负载均衡服务，然后通过域名解析配置，将流量分别导向三个不同的服务机从而达到上述的需求。用一句话解释看起来好像我们自己做也挺简单的，但是，如果我们只有两台服务器呢？如果有一台中的 container 挂了呢？如果两台服务器 CPU 跑满了呢？这种调度方面的东西看起来很简单，但是实现起来却需要长时间的编码和调试，并且最终做出来可能也就是个 Docker Swarm 而已。

我们把这个需求看做是我们最终的目标，先让我们来一步一步看看 kubernetes 中关于编排方面的处理，相信大家看完这部分内容之后，就能很容易完成上述的需求了。

容器编排在 kubernetes 中其实就演变成了 Pod 编排，怎么让这些 Pod 配合起来协同工作就是编排的核心了。上一节我们知道了，kubernetes 将各种关系进行了抽象化，这些关系其实可以看做是 Pod 之间的关系。kubernetes 将 Pod 的关系抽象成了一下几种，并且为这些关系定义了相对的控制器进行编排管理：

- 无状态 Pod 副本之间的协同关系——Deployment
- 有状态 Pod 副本之间的拓扑关系——StatefulSet
- 容器化守护进程——DaemonSet
- 离线业务——Job 和 CronJob

这些概念看起来云里雾里不知所云，什么有状态无状态的，其实就是通过上述的控制器对 Pod 的不同管理方式而已。这里限于篇幅和个人水平，我只讲一下我们云组使用到的也是 kubernetes 最常用的一种控制器 Deployment。

Deployment 控制器的功能是：维护多个相同的无状态 Pod 副本`以规定的数量运行`，并且支持`水平扩展`以及`滚动更新`。所以，我们最终需求中的负载均衡中服务的 Pod，就可以通过 Deployment 管理，我们可以通过 Deployment 让我们的 Pod 在 kubernetes 集群中始终以 3 个副本的形式存在。

这里引出了一个概念：`控制器`，控制器是 kubernetes 中管理待编排对象的程序，其把这种`一个对象管理另一个对象的模式称为控制器模式`，kubernetes 中的所有待编排对象都是通过控制器模式管理的。其核心就是一个死循环，在循环中不停地判断当前编排对象的状态，如果不满足预期状态就更新它，如下的伪代码就是描述一个控制器的工作原理：

```go
for {
  实际状态 := 获取集群中对象X的实际状态（Actual State）
  期望状态 := 获取集群中对象X的期望状态（Desired State）
  if 实际状态 == 期望状态 {
    什么都不做
  } else {
    执行编排动作，将实际状态调整为期望状态
  }
}
```

因此我们知道了，如果定义我们的 Pod 使用 Deployment 来编排，并且要求副本数量是 3，那 Deployment 控制循环中就会不停地判断我们的 Pod 的副本数量是否是 3，如果不是，就会触发水平扩展功能进行调整，最终达到满足期望状态（副本数==3）。

说了这么多，我们可以通过一个栗子看看 Deployment 是如何工作的。由于应用的镜像配置过于复杂，因此这里我们通过一个 Nginx 的多副本配置来感受一下 Deployment 控制器的控制结果，我们可以通过以下 yaml 定义一个维护了 3 个 nginx 副本的 deployment：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-deployment-nginx
spec:
  selector:
    matchLabels:
      app: sample-deployment-nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: sample-deployment-nginx
    spec:
      containers:
        - name: sample-nginx
          image: nginx:1.9.1
          ports:
            - containerPort: 80
```

这里我们看到了三个特殊字段：

1. selector：是一个选择器，就可以理解为 js 中的选择器，其功能就是选择指定的 pod 运行，这个栗子中我们指定所有 app==sample-deployment-nginx 的 pod 才会被这个 Deployment 所部署
2. replicas：指明这个 Deployment 维护的副本个数
3. template：控制器中提供了 template 这个语法，可以让我们直接在控制器的 yaml 中直接编写所需要编排的 Pod 信息

编写完这个 sample-deployment-nginx.yaml 后，执行一下：

```shell
kubectl apply -f sample-deployment-nginx.yaml
```

这个三副本的控制器就被成功运行了，我们可以通过这个命令进行查看：

```shell
kubectl get pods -l app=sample-deployment-nginx
```

![deployment_pods_running](https://image.dunbreak.cn/cloud/deployment_pods_running.png)

可以看到我们的 3 副本 Pod 已经成功在 kubernetes 中运行了，此时，如果你执行以下命令删除 podname==sample-deployment-nginx-54545f95cd-wtllm 的副本：

```shell
kubectl delete pod sample-deployment-nginx-54545f95cd-wtllm
```

它还是会自动生成一个新的 pod 来维持这个 replicas==3：

![deployment_pods_recovering](https://image.dunbreak.cn/cloud/deployment_pods_recovering.png)

通过上述例子，我们可以看到 Deployment 控制器对副本数量的控制结果，其实，对副本数的控制是 ReplicaSet 控制器的结果，Deployment 是 ReplicaSet 控制器的控制器，这种多层之间相互控制的模式在 kubernetes 也十分常见，其之间的关系如下图所示：

![deployment_replicaSet_pods](https://image.dunbreak.cn/cloud/deployment_replicaSet_pods.jpg)

此时，我们再来看水平扩展/收缩，我们可以执行以下命令，分别将这个 deployment 对象的信息修改成 5/0，并且最终改为 3：

```shell
kubectl edit deployment/sample-deployment-nginx
```

我们可以看到如下结果，deployment 将我们收缩的成 0 的 5 个 pod 副本都删除并且重新创建了 3 个新的副本：

![deployment_editing](https://image.dunbreak.cn/cloud/deployment_editing.png)

至此，水平扩展和收缩的功能就已经成功了，最后我们来看看滚动更新，我们继续通过 edit 命令将 nginx 的版本从 1.9.1 改为 latest，我们可以看到一个滚动更新的过程：

![deployment_updating](https://image.dunbreak.cn/cloud/deployment_updating.png)

也正是这样逐步更新删除的动作，被称为滚动更新，其更新过程中，deployment 和 replicaSet 的关系如下图：

![deployment_replicaSet_pods_updated](https://image.dunbreak.cn/cloud/deployment_replicaSet_pods_updated.jpg)

至此，一个 deployment 管理 pod 的所有功能都已经展示完成了，可以看到 kubernetes 对这种控制器管理之间精妙的设计，多个控制器协同工作，这样既能保证精准也能拆分进程阻塞提升性能。当你理解了 deployment 控制器，对于其他的控制器来说也就很容易理解了，在此我来简单说明一下其他控制器的控制逻辑：

1. StatefulSet：控制满足有拓扑状态或者持久化存储的 Pod，拓扑状态的意思就是 Pod 之间存在明确的先后生成关系，持久化存储就是当副本被删除或者修改了，其内部保存的数据还会存在
2. DaemonSet：守护进程控制器，是一个 Node（服务器节点）仅能存在一个的 Pod，比如系统的日志采集器等就应该用这种方式调度
3. Job 与 CronJob：Job 就是任务调度，一个 Pod 在调度完成后就结束了不会再有新的任务产生，Job 用于维护一个任务 Pod 运行中的各种状态正常，异常状态重启等。对应的 CronJob 就是定时任务，使用过 Quartz 的肯定不陌生

综上，kubernetes 中就是通过上述的各种控制器维护所有 Pod 的编排工作的，并且其还提供了完善的 API 可以让用户自行定义满足自己需求的各种 Pod 编排控制器。但是对于 deployment 我只是简单的展示了一些常用的功能点，其内部还有滚动更新的最大资源、金丝雀发布和灰度发布等各种功能需要细致的学习。

### 我的服务在哪？

上面我说了，需要完成我们最终发布应用的需求，编排只是其功能的一部分，而另一部分就是服务的发现机制了。目前，我们仅仅是把我们的服务发布在了 kubernetes 集群中，但是要想内部交互，外部调用完成`人与狗的交往关系`还缺少服务发现这个功能。服务发现这个功能做过微服务的同学肯定了解过，spring cloud 项目中的 Eureka 框架就是完成这个功能的，其主要工作就是注册内部的服务，以供其他集群的服务可以调用访问这个服务。

kubernetes 中野存在这样的功能（而且很有可能是 kubernetes 现有才激发了各种微服务框架产生服务发现机制），对应的模块是 Service 与 Ingress，我们来分别说这两个功能。

Service 类似于服务的注册功能，其逻辑很简单，在 kubernetes 声明一个服务，从而生成一个 VIP（虚拟网络），所有 Kubernetes 集群中的其他组件，都可以通过这个 VIP 来访问这个服务，并且这个服务是不会随 Service 的改变而改变的，只要创建就是终生存在。而服务的内容是什么呢？其实和上述的 Deployment 一样，是通过 selector 选择器确定的。我们可以通过下述 yaml 来创建一个服务：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: hostnames
spec:
  selector:
    app: hostnames
  ports:
    - name: default
      protocol: TCP
      port: 80
      targetPort: 9376
```

通过上一节的讲解，我们能知道这个服务所需要的代理的内容是 app==hostnames 的 Pod，并且这里也有一个新的字段`ports`，这个字段是说明该代理的服务的请求方式（protocol）、对外暴露的端口（port）、内部的端口（targetPort）分别是什么。

我们可以通过这个 sample-service.yaml 的文件创建一个 Service 并且查看一个 Service：

```shell
# 创建
kubectl apply -f sample-service.yaml

# 查看
kubectl get services hostnames
```

![service_created](https://image.dunbreak.cn/cloud/service_created.png)

可以看到这个 service 存在一个 ClusterIP，这个 IP 就是这个 Service 生成的 VIP，在集群内部的其他成员，都可以通过这个 VIP 来访问这个 Service。当然，由于我们现在没有任何的具体服务让这个 Service 代理，因此现在请求这个 IP 是不会成功的。

接下来，我们为这个 Service 创建一个具体实现：以下的 sample-deployment.yaml 文件是创建一个多副本的 Pod，其 Pod 的功能是返回自己的 podname：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hostnames
spec:
  selector:
    matchLabels:
      app: hostnames
  replicas: 3
  template:
    metadata:
      labels:
        app: hostnames
    spec:
      containers:
      - name: hostnames
        image: k8s.gcr.io/serve_hostname
        ports:
        - containerPort: 9376
          protocol: TCP
~
```

可以看到，我们把容器的 9376 端口暴露的出来，以为这个 Pod 是通过这个端口与外部通行的。同样我们执行以下命令创建和查看这个 pod 副本：

```shell
# 创建
kubectl apply -f sample-deployment.yaml

# 查看
kubectl get pods -l app=hostnames
```

![service_deployment_created](https://image.dunbreak.cn/cloud/service_deployment_created.png)

可以看到这个 pod 副本已经创建成功了，此时，根据我上一节所说的控制器模式，Service 也有对应的处理 Service 的控制器，其在内部发现了有满足 app==hostnames 的服务，即将这个服务和 Service 进行了绑定，此时，我们就可以通过任意一台集群内的主机来请求刚才那个 ClusterIP 了：

![service_requesting](https://image.dunbreak.cn/cloud/service_requesting.png)

可以看到，我请求了多次，但是每次返回的结果不同，这是因为 Service 在其内部通过网络插件（CNI）做了负载均衡处理，所以我们最终需求的负载均衡功能，就可以通过这个 Service 来实现。

在此要说一个我之前一直的误解：我之前一直以为 Service 是必须对应 Deployment 这种 Pod 的编排控制器对象才能工作的，所以把 Service --> Deployment --> Pods 这条逻辑关系熟记于心，但是其实是**错误的**。在 Kubernetes 中，每个功能组件各司其职，他们只会处理自己该做的事，比如这里，Service 绑定 Pod 所依赖的是选择器中的 app==hostnames，而这个定义是在 Deployment 中定义在 Pod 中的，因此 Service 和 Deployment 完全没有关系，它们俩谁也不认识谁，关系可以用下图来描述：

![service_deployment](https://image.dunbreak.cn/cloud/service_deployment.png)

并且，之前还错误的任务负载均衡服务是由 Deployment 提供的，其实这个功能是 Service 中的网络插件来处理的，并且用户同样也可以自定义使用的网络查件或者负载均衡算法是什么，Kubernetes 给了用户足够大的自由度。

在有了 Service 之后，我们的服务就可以在集群中随意访问达到服务之间的交流关系了。但是要想让我们的服务让最终的用户访问到，我们还需要最后一个组件 Ingress。Ingress 是 Kubernetes 中的反向代理服务，它可以解析配置的域名指向到我们内部的 Service 中，其定义可以通过下述的 yaml 来实现：

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: sample-ingress
spec:
  rules:
    - host: hostname.sample.com
      http:
        paths:
          - path: /
            backend:
              serviceName: hostnames
              servicePort: 80
```

可以看到，我们将`hostname.sample.com`这个域名指向了我们刚才定义的`hostnames`这个 Service，这样，我们的服务就可以通过定义的域名配置供外部的服务进行访问了。而 Ingress 的创建命令，也和前面所说的一样：

```shell
kubectl apply -f sample-ingress.yaml
```

有了这部分配置，我们的功能**原则上**就能够让外部访问了，这里加了原则上，是因为我本地没有可供测试的环境，本地的 Kubernetes 环境是通过 kindD 生成的，其核心不是多台机器而是多个 Docker Container，其在 Container 内部运行 Docker 模拟了 Kubernetes 的功能，因此这也是我全文中**唯一没有验证成功的一个功能模块**。

至此，整个 Kubernetes 的基础使用的流程就已经介绍完毕了。我们可以看到一个服务在 Kubernetes 变成 Pod，通过 Deployment 部署，通过 Service 服务发现，通过 Ingress 反向代理的全过程，经过这些模块的协力配合之后，我们的应用终于可以部署在这个 Kubernetes 集群中了。

### Kubernetes 总览

现在，让我们在回过头来看看 Kubernetes 的内部组件图，我想大家就应该了然于胸了：

![kubernetes_modules](https://image.dunbreak.cn/cloud/kubernetes_modules.png)

还有几个我没有提到的功能模块：

- ConfigMap 是用来存储用户配置文件定义的，通过其内部的 Volume 投射技术实现，其实也是 Volume 挂载的一种方式，感兴趣的同学可以自己查看
- Secret 是用来定义密码类型数据的，同样通过 Yaml 文件的形式定义在 Kubernetes 集群中，供需要使用到的 Pod 通过 secretName 的方式使用
- PV，PVC 是 Kubernetes 中持久化数据卷的实现方式，它是 StatefulSet 的核心功能，也是 Pod 持久化的必要手段，Kubernetes 通过 PV 和 PVC 拆分，达到了功能点的解耦，这里面也有很多的功能点，感兴趣的同学可以自己在官方文档中学习

其实，Kubernetes 集群远比我们目前看到的复杂的多的多，其内部还有大量的功能由于我们不会使用所以我在之前没有介绍到，在这里，我对这些深层次的功能做一些总结，也对深入学习 Kubernetes 开一个头。

#### Kubernetes 组件

首先，我们要知道 Kubernetes 是有一个或者多个`Master节点`控制多个宿主机实现集群的。其中的宿主机就是我们平时开发中的服务器，在 Kubernetes 集群中被称为`Node节点`，而整个 Kubernetes 的核心调度功能基本都在 Master 节点上。Kubernetes 的主要功能通过五个大组件组成：

1. kubelet：安装在 Node 节点上，用以控制 Node 节点中的容器完成 Kubernetes 的调度逻辑
2. ControllerManager：是我上述所讲的控制器模式的核心管理组件，管理了所有 Kubernetes 集群中的控制器逻辑
3. API Server：服务处理集群中的 api 请求，我们一直写的 kubectl，其实就是发送给 API Server 的请求，请求会在其内部进行处理和转发
4. Scheduler：负责 Kubernetes 的服务调度，比如控制器只是控制 Pod 的编排，最后的调度逻辑是由 Scheduler 所完成并且发送请求给 kubelet 执行的
5. Etcd：这是一个分布式的数据库存储项目，由 CoreOS 开发，最终被 RedHat 收购成为 Kubernetes 的一部分，它里面保存了 Kubernetes 集群中的所有配置信息，比如所有集群对象的 name，IP，secret，configMap 等所有数据，其依靠自己的一致性算法可以保证在系统中快速稳定的返回各种配置信息，因此这也是 Kubernetes 和心中的核心组件

#### 定制化功能

其次，Kubernetes 给用户提供了极高的自由度，从上述的各种栗子都有不同的展现。其实，Kubernetes 给用户提供了三个公开的接口，分别是：

1. CNI：Container Networking Interface，容器网络接口，其定义了 Kubernetes 集群所有网络的链接方式，整个集群的网络都通过这个接口实现。只要实现了这个接口内所有功能的网络插件，就可以作为 Kubernetes 集群的网络配置插件，其内部包括宿主机路由表配置、7 层网络发现、数据包转发等等都有各式各样的小插件，这些小插件还可以随意配合使用，用户可以按照自己的需求随意定制化这些功能
2. CSI：Container Storage Interface，容器存储接口，定义了集群持久化的一些规范，只要是实现这个接口的存储功能，就可以作为 Kubernetes 的持久化插件
3. CRI：Container Runtime Interface，容器运行时接口，其实现是 Kubernetes 的容器运行时，比如默认配置的 Docker 就是这个集群的容器运行时，用户可以自由选择实现了这个接口的其他任意容器项目，比如之前提到过的 containerd 和 rkt

这里还要重点说一下 CRI，Kubernetes 的默认容器是 Docker，但是由于项目初期的竞争关系，Docker 其实并不满足 Kubernetes 所定义的 CRI 规范，那怎么办呢？为此，Kubernetes 专门为 Docker 编写了一个叫`DockerShim`的组件，即 Docker 垫片，用来把 CRI 请求规范，转换成为 Docker 操作 Linux 的 OCI 规范（对，就是第二部分提到的那个 OCI 基金会的那个规范）。但是这个功能一直是由 Kubernetes 项目维护的，只要 Docker 发布了新的功能 Kubernetes 就要维护这个 DockerShim 组件。于是，也就是这个月初的消息，[Kubernetes 将在明年的版本 v1.20 中删除删除 DockerShim 组件](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/)，这也意味着，从明年的新版本开始，Kubernetes 将全面不支持 Docker 容器的更新了。当然，这对我们普通开发者来说可能并没有什么影响，可能最坏的结果就是我们的镜像需要从 Docker 换成其他 Kubernetes 支持的容器镜像。不过根据这几天看到的各个云平台所说的消息，他们都会提供对应的转换措施，比如我们还是提供 Docker 镜像，平台在发布运维的时候会把这些镜像转换成其他镜像，又或者他们会自行维护一个 DockerShim 来支持 Docker，总之是由解决方案的。但是这个消息同时也告诉了我们，`Docker离灭亡已经不远了`。

#### 架构总览与总结

最后，让我们看看 Kubernetes 的架构图：

![kubernetes_architecture](https://image.dunbreak.cn/cloud/kubernetes_architecture.png)

通过这一系列的学习，我作为一个普通程序员，不得不赞叹 Google 这种公司对编码这件事得深厚与极致，框架中因为太多仅因为解耦而产生的组件，并且还提供了这么大的自由度，不得不说是我工作以来学习过的最高端的一个框架。

但是也不得不说明一点，过高的自由度也有负面作用，就是 Kubernetes 集群部署的复杂度非常高，部署一个满足生产环境需求的 Kubernetes 框架更是难上加难，网上还有专门卖 Kubernetes 生产环境集群部署的脚本程序，可见 Kubernetes 系统的庞大。当然作为普通开发者，我们学习的时候可以使用 kinD 或者 minikube 在本地以 Docker 的形式模拟一个 Kubernetes 集群，但是离生产环境还差的很远。

## 总结

这篇文档，详细的描述了我们云组上天过程中碰到的几个难啃的神仙。并且我一步一步抽吸剥茧地向大家解释了一个云平台，从最初的虚拟机，到 PaaS 雏形，到 Docker 容器化，再到最终 Kubernetes 的形态的过度和演变。我认为人的记忆是需要依赖于前驱节点的，我不想仅仅通过一篇讲述 Kubernetes 技术点的文章来一个一个解释 Kubernetes 中那些难记的名词，而是想让大家一步一步了解整个云生态的演化过程，从而最终理解整个项目。

最后想送给大家一句话：

> 纸上得来终觉浅，绝知此事要躬行

我在第一遍看完这些文档之后觉得自己已经啥都懂了，但是等到自己要写文档的时候真是两眼一抹黑，太多的知识点只停留在听说过不知道是什么的阶段，因此还是建议大家动起手来，一步一步操作一下里面的实例，我相信在自己动手写过一遍之后，你会有不一样的理解~

## 参考文档

- 极客时间：[深入剖析 Kubernetes](https://time.geekbang.org/column/intro/100015201)、[Elasticsearch 核心技术与实践](https://time.geekbang.org/course/intro/100030501)
- CoolShell：[Linux PID 1 和 Systemd](https://coolshell.cn/articles/17998.html)、[DOCKER 基础技术：LINUX NAMESPACE（上）](https://coolshell.cn/articles/17010.html)
- Wikipedia：[文件系统层次结构标准](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard)
- [Linux 线程实现机制分析](https://www.ibm.com/developerworks/cn/linux/kernel/l-thread/index.html)
- [YAML 语言教程](http://www.ruanyifeng.com/blog/2016/07/yaml.html)
- [《Kubernetes 与云原生应用》之容器设计模式](http://www.dockerinfo.net/2067.html)
