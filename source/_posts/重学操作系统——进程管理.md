---
title: 重学操作系统——进程管理
url_name: os-process
date: 2021-10-02 08:21:14
categories:
  - 操作系统
tags:
  - 操作系统
  - 基础知识
---

这一节开始重新学习操作系统的进程管理。当我学完这章之后发现进程管理的内容是如此的纷乱繁杂，内容概念多，需要的算法实现更多。因此建立对整体概念的知识体系很重要，这篇内容应该以抽丝剥茧的形式一层一层深入理解。

<!-- more -->

## 进程的产生

在最开始的操作系统中其实是没有进程的概念的，计算机处理的都是一个一个的程序。但是随着计算机的发展，计算机内可以运行的程序越来越多，**一个程序也可以运行多份**，这个时候再用程序来描述就不够用了，因为你并不能把两个同时运行的程序当做是相同的，虽然两个程序的执行代码相同，但是由于时间的不同，执行的数据不同等因素，最终两个程序的直接结果可能完全是不相同的，为此出现了进程管理。

程序的状态分两种，一种是进程的静态程序，就是存在硬盘中的程序，一个是进程运行动态执行的过程。可以用 **Docker Image** 和 **Docker Container** 来类比对应的关系（虽然有点本末倒置，但是我理解中其原理是类似的）。因此进程就是一个具有一定独立功能的程序在一个数据集合上的一次动态执行过程。并且进程执行有**核心态**和**用户态**的区分，**核心态就是在内核中执行的进程**。

### 进程控制块

一个进程的运行只需要代码是不够的，还需要：

1. 程序的代码，因为进程的执行完全由代码控制
2. 程序处理的数据集合，进程运行是需要一定的程序资源
3. 程序计数器，程序计数器用来指示当前程序运行的下一条指令
4. 寄存器，程序的运行需要大量的寄存器，内部存放堆栈数据
5. 一组系统资源，比如打开文件

而这些资源时需要一个内存模块进行存储的，即`进程控制块 PCB（Process Control Block）`。PCB 就是用来描述进程的数据结构，操作系统为每一个进程都维护了一个 PCB，用来保存与该进程有关的各种状态信息以及运行变化的过程，是进程存在的唯一标志。PCB 含有以下三大类信息：

1. 进程标识信息。如本进程的标识，本进程的产生着标识（父进程）；用户标识
2. 处理机状态信息保存区。保存进程的运行现场信息，主要有：
   1. 用户可见寄存器，用户程序可以使用的数据，地址等寄存器
   2. 控制和状态寄存器，如程序计数器，程序状态字
   3. 栈指针，过程调用/系统调用/中断处理和返回时需要它
3. 进程控制信息，主要有：
   1. 调度和状态信息，用于操作系统调度进程并占用处理机使用
   2. 进程间通信信息，为支持进程间的与通信相关的各种标识、信号、信件等，这些信息存在接收方的 PCB 中
   3. 存储管理信息，包含有志向本进程影像存储空间的数据结构
   4. 进程所用资源，说明由进程打卡、使用的系统资源，如打开文件等
   5. 有关数据结构连接信息，进程可以连接到一个进程队列中，或连接到相关的其他进程 PCB

PCB 一般可以通过链表和索引表组织，但实际场景中进程是动态运行的，会频繁增减，因此为了更好的性能一般使用链表组织。`进程间有很好的隔离性`，每个进程都有自己独有的 PCB，并且 PCB 中的资源信息比如堆栈，内存空间，寄存器，程序计数器等资源都是`相互隔离`的，相互之间无法直接访问，只能通过 IPC 的方式进行消息通信或者共性资源访问。

### 进程的生命周期

进程的生命周期分为：**创建**、**运行**、**等待（阻塞）**、**唤醒**、**结束**。其主要的状态变化可以从下图看到：

![process-state](https://image.dunbreak.cn/os-process/process-state.png)

> 注意，阻塞和挂起时两个不同的概念，我之前一直以为阻塞就是挂起，很丢人。如果一个进程处在挂起状态，那说明进程`没有占用内存空间`，处在挂起状态的进程映射在磁盘中。

根据上面的定义我们可以知道，挂起就是把一个进程从内存转到外存（结合虚拟内存技术）。挂起主要分为以下两类：

1. 阻塞挂起状态（Blocked-suspend）：进程在外存并等待某时间的出现
2. 就绪挂起状态（Ready-suspend）：进程在外存，但只要进入内存，即可运行

加入了挂起状态后，进程的状态变化图可以描述为：

![process-state-with-suspend](https://image.dunbreak.cn/os-process/process-state-with-suspend.png)

具体的挂起状态存在以下几种情况：

1. 阻塞到阻塞挂起：就是把处在阻塞状态的进程挂起，当*没有进程处在就绪状态* 或 _就绪状态的进程需要更多内存资源_，会进行这种转换，以*提交新进程* 或 _运行就绪进程_
2. 就绪到就绪挂起：默认情况下，就绪态进程是比阻塞态进程优先级高的，因此会默认优先挂起阻塞态的进程。但是如果*有高优先级阻塞进程和低优先级就绪进程时*，系统会选择挂起低优先级的就绪进程
3. 运行到就绪挂起：对*抢先式分时系统*，当有高优先级阻塞挂起进程因事件出现而进入就绪挂起时，系统可能会把运行进行转到就绪挂起状态
4. 阻塞挂起到就绪挂起：注意这种情况只会出现在外存中。如果挂起状态的阻塞进程*接收到了相应的事件需要被转换为就绪状态*，但是目前还没有资源分配给它时，会把阻塞挂起的进程转换到就绪挂起状态
5. 就绪挂起到就绪：当*没有就绪进程* 或 *挂起的就绪进程优先级高于就绪进程*时，会进行这种转换
6. 阻塞挂起到阻塞：当一个进程释放内存时，系统会把一个*高优先级阻塞挂起进程*（系统判断即将出现等待事件的进程）转换为阻塞状态

为了更好的管理这些状态，操作系统一般使用状态队列，来表示系统挡住所有进程的当前状态。不同状态分别会用不同的队列表示，比如就绪队列，各种类型的阻塞队列。每个进程的 PCB 都根据它的状态加入到相应的队列中，当状态变化时会从一个队列脱离到对应的队列中。并且，各种状态的队列也存在多个，因为需要分优先级。

## 线程的出现

自动 20 世纪 60 年代进程的概念出现后，操作系统一直都是以进程作为独立运行的基础单位的。知道 80 年代中期，人们又提出了更小的能独立运行的基础单位，即线程。

那么，在已有进程的情况下，为什么又会提出线程呢？

举一个例子：现在需要编写一个播放 MP3 的程序，那程序的伪代码可以为：

```cpp
main() {
  while(true) {
    /*IO读取音频流*/
    Read();
    /*解压音频流*/
    Decompress();
    /*播放*/
    Play();
  }
}

Read() {...}
Decompress() {...}
Play() {...}
```

那这样的程序在单进程中会有啥问题吗？

有的，如果在单进程中，程序 I/O 会占用大量时间，CPU 很有可能就把这个进程阻塞了。那很有可能刚开始就完全听不到声音或者声音是断断续续的（因为 IO 每一次就是一个 buffer 的 size，读完之后去解压，然后播放，然后再读，第二次读的时候就会有明显的卡顿），主要原因是各个方法之间不是并发的，是串行的。

那是否可以用多进程来解决这问题？

我们可以把里面的每个方法写到一个进程中，让它们依次地工作执行，比如 Read 进程先去读取音频文件，之后将数据传输给 Decompress 进程，Decompress 工作之后将解码音频文件交给 Play 进程去播放。这样做确实可以解决并发执行的问题，但是由引入了新的问题，即进程间通信的问题。学习了后续的 IPC 之后我们就可以知道，进程间通信的代价还是很大的，并且如何维护三个进程之间有效的执行切换保存状态信息等也是一个很复杂的调度问题。

为此，一个可以`并发执行`并且可以`共享地址空间`和`资源`的实体线程就出现了。

有了线程之后，进程的概念就有一点变化了。**进程用来管理**程序执行过程中的一系列**资源**，而**程序的执行**被拆分出来给了**线程**。从而进程是资源分配单位，线程是 CPU 调度单位。此外，线程同样具有**就绪**、**阻塞**和**执行**三种状态，并且也具有状态转换关系。

线程对应于进程也有自己的`线程控制块 TCB（Thread Control Block）`，其内部存放的是线程执行中需要的一些资源信息，包括：**程序计数器**，**栈**，**寄存器**，**状态信息**等。

![relationship-of-process-thread](https://image.dunbreak.cn/os-process/relationship-of-process-thread.png)

线程的出现，不光能解决地址空间共享的问题，并且还能显著地减少并发执行 时间 和 空 的开销，其主要原因是：

1. 线程的创建时间比进程短，因为进程创建过程中，还需要创建内存管理、文件管理等内容，而线程创建可以直接重用进程所属的这些已经创建好的资源
2. 线程的终止时间比进程短，同理不需要考虑上述资源的释放问题
3. 同一个进程内的线程切换时间比进程切换时间短，因为同一个进程内的程序具有相同的页表，因此线程切换并不需要切换页表，而进程切换则需要。而切换页表的开销是比较大的，包括需要切换一些 cache 信息，TLB 的信息等
4. 同一进程的各个线程共享内存文件资源，可直接通信，不需要通过内核通信

但是，虽然线程的优点是可以共享地址空间，但是其缺点也正是可以共享地址空间。因为如果一个线程写错了，可能会导致整个进程内的地址空间错误，从而导致整个进程挂掉。举一个例子：早起的浏览器大多使用线程的方式实现的。但是使用线程就存在如果一个网页存在恶意代码导致当期线程出现问题，那么整个浏览器内的其他网页都会出现问题。为此现在的浏览器大多都是用进程实现的比如 chrome。

因此，到底使用进程还是线程，其主要取决于安全性的考虑。

### 用户态线程

线程其实可以分为**用户线程**和**内核线程**。这两个分类其实也很好理解，在用户态实现的线程就是用户线程，其对操作系统内核其实是透明的，操作系统感知不到用户线程的存在，其调度的对象其实还是整个进程。

> 其实在早期，操作系统内核并没有线程这个概念的，当时的线程都是通过用户库实现的，比如 POSIX Pthreads（网络的功能模块也是这样先有用户库后成为操作系统的一部分的）。

一般的用户线程是用户库在应用程序内部构造相应的线程控制块来控制一个进程内部的多个线程交替执行和同步，因此，用户态线程需要用户库自行实现一套完整的线程调度系统。

用户态线程的优点就是`创建成本低`以及`切换速度快`。

- 关于创建成本低：因为从上一节的内容我们已经知道了，线程其实也包括一个 TCB，其虽然共享地址空间，但是还需要为其创建独占的**程序计数器**，**栈**，**寄存器**，**状态信息**等资源。而用户级线程是用户自己创建的，并不需要为其创建程序计数器和栈，只需要一个栈指针就可以标记用户态线程的堆栈信息了，因此创建成本低。
- 关于切换速度快：这一点也很好理解，因为如果是系统线程进行切换，则需要从用户态切换到内核态，并且操作系统需要在内核态中的所有线程选定一个符合调度算法的线程进行切换，如果切换到了一个其他进程内的线程还需要考虑切换页表调度内存等一系列的问题。而用户线程切换完全在用户态内部实现，仅仅改变栈指针即可。

> 进程切换的时间间隔是 10 毫秒，为了达到最高的切换效率，通常切换的逻辑都是由汇编实现

当然用户线程也有缺点：

1. 因为操作系统无法感知到用户线程，因此用户线程**无法实现抢占**其他线程的 CPU，因为用户态无法调用中断处理程序
2. 操作系统分配调度时间是以进程的形式分配的，如果进程内有自己的多个线程，那每个用户线程的调度时间就会更少
3. 如果用户线程执行了 IO 操作被系统阻塞，那么该用户线程对应的整个进程都会被阻塞

> 这里说到的抢占，其实是进程调度的一种策略，抢占式的进程可以打断正在执行的其他进程，这个功能一般都是内核提供的。早期的操作系统进程都是非抢占式的，逐步的出现了用户态抢占式进程，逐渐到现代操作系统中，内核态进程也可以支持抢占，从而让系统更加的灵活高效

为了更好的解决抢占的问题，在用户态线程的基础上又出现了`协程 coroutine`的概念，协程可以通过 **yield（取其“让步”之义而非“出产”）**来调用其它协程，接下来的每次协程被调用时，从协程上次 yield 返回的位置接着执行，通过 yield 方式转移执行权的协程之间不是调用者与被调用者的关系，而是彼此对称、平等的。

Golang 中实现并发变成的`goroutine`就是一个自行实现调度系统的协程。

而针对调度时间短以及 IO 操作阻塞的问题，也通过了用户线程与系统线程多对多的建模而很好地解决了。

上述所说的通常是最普通的用户态线程，即一个系统线程对应多个用户态线程。但是对于现在协程等用户态线程来说，基本都是多对多的关系，如果一个用户态线程执行了 IO 操作，用户态调度系统会将对应的用户态线程切换到其他线程中继续执行。

[用户级线程到底有什么用？](https://www.zhihu.com/question/307787570)这个帖子中很好的说明了用户态线程的一些问题的解决方案，也使得协程这个概念能在 Golang 中大行其道。学完了用户态线程这一章，我也基本可以解答之前提出的 [问题一](https://www.dunbreak.cn/2021/09/20/os-basics/#%E4%B8%80%E3%80%81%E4%B8%BA%E5%95%A5%E9%83%BD%E6%98%AF%E8%AF%B4-Golang-%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E4%BB%A3%E4%BB%B7%E5%BE%88%E5%B0%8F%EF%BC%9F) 了，这一小结的所有内容基本就是这个问题的答案。

### 线程创建

上一小节算是一个小插曲，主要说的是用户态线程的一些概念。而相对的，本节所说道的线程创建，基本都是系统线程的创建。

系统线程创建目前基本都是通过 kernel api 的形式完成的。Windows 下调用`CreateProcess(fileName)`，Unix 下调用`fork/exec`。由于 Windows 不开源，我们无法知道这个 api 内部究竟做了些什么，为此这里主要说一下 Unix 下的 fork 和 exec 到底在创建线程的时候干了哪些事。

> 要说明一点：Unix 系统下创建进程和线程都是通过这种方式实现的，因此下面的逻辑同样也是创建进程的逻辑。

fork 这个函数和我们日常用 git 所使用的 fork 基本相同，就是**将一个进程完全复制出新的一份**，注意这里的**完全**，它不光会复制进程的代码段，包括进程的地址空间、页表、寄存器等一系列资源都会复制一份。

当调用 fork 之后，操作系统内已经有两个完全一样的进程了，并且指令寄存器都指向了 fork 之后的那一行代码。其区别是父进程执行 fork 完成返回值是子进程的 pid，而子进程调用返回的 pid==0，为此就分出了两个不同的分支执行不同的逻辑：

```cpp
int pid = fork();
if (pid == 0) {
    /* 子进程的逻辑: Do anything (unmap memory, close net connections ...) */
    exec("program", argc, argv0, argv1, ...);
    printf("This code will not be executed");
} else {
    // 父进程的逻辑
    wait(pid);
}
```

而 exec 是 fork 后 99%都会调用的函数，其作用是**将子进程的代码段和内存等资源加载进子进程 fork 出的资源段内**并且**覆盖**，之后执行对应程序的 main 函数。例如上面的那段代码中 exec 就是找到`program`这个进程的 main 函数执行。

此外，我们可以看到 exec 函数之后会输出一个 printf，而如果子进程调用成功的话这个 printf 是不会执行的。因为 exec 会完全覆盖子进程的代码段，所以执行了 exec 之后的子进程完全变成了一个新的程序。

不过这里就有一个问题了：既然 exec 函数会将子进程的代码和内存覆盖，那么 fork 复制的那一份工作不是多余了吗？有什么方法可以解决这个问题吗？

一般有两个解决方案：

1. 早期 Unix 系统中一般调用 vfork()，该函数只创建一个进程而不创建对应的内存
2. 使用 COW 技术

第一种就不过多介绍了，因为这一种方案又重新写了一份阉割版 fork 的逻辑，并且随着系统的升级还带来了维护的工作，因此大部分系统都是使用 COW 技术。而这里所说的 COW 技术其实是结合了上一篇内存管理所使用到的虚拟内存技术实现的。

COW 就是 Copy on Write，即在父进程调用 fork 复制时，并没有直接复制自己的地址空间，而是只复制了地址空间所用到的**元数据**即**页表**，当父进程或者子进程对页表中的某个地址进行写操作时，会触发一个系统异常，该异常中会把对应的页复制成两份。

最后，上述例子中如果是父进程的逻辑，那最后会调用一个`wait(pid)`的函数，这个函数是为了释放子进程的 PCB。因为上面说的 PCB 是一个进程存在的唯一标识，子进程虽然可以在自己执行结束之后将所用的资源释放，但是它自己没办法释放自己的 PCB，因此需要父进程调用 wait 来帮助子进程完成这个逻辑。

一个进程直接结束后一般需要调用 exit 这个系统调用，然后操作系统会在内核态将刚才进程的一些系统资源进行释放和关闭。但是这个 exit 执行结束后进程就无法回到自己的用户态继续释放 PCB 了，因此需要父进程协助子进程释放 PCB。在一个进程调用 exit 并且 PCB 没有释放时，该进程会变成一个**僵尸（zombie）进程**加入僵尸队列中。加入了僵尸进程这个概念之后，整个进程的状态变化可以由下图展示：

![process-state-with-zombie](https://image.dunbreak.cn/os-process/process-state-with-zombie.png)

当然，也有可能父进程先于子进程调用结束。这种情况一般由操作系统创建的初始进程即 root 进程定时扫描僵尸队列并释放它们。

## 调度算法

上两节已经很好的说明了计算机中进程和线程的一些概念。而进程管理的核心和上一章内存管理一样，就是协调管理调度这些计算机内的进程，让它们能够平稳高效的执行。一个进程只要满足了：从运行状态切换到*等待状态* 或 _被终结了_，就可以被调度。

调度算法的原则尽量增加 CPU 的吞吐率。拿单一进程来说，一个进程在运行过程中，其 CPU 使用率是波动的，这也很好理解，需要大量计算时 CPU 使用率高，当进程需要 IO 操作时，CPU 不工作可能处在等待状态因此使用率低。但这只是单一进程的情况，而调度算法就是为了提高整个系统的 CPU 吞吐率，使得 CPU 的等待时间尽可能的少。

> 但是，目前移动端的操作系统可能不是将使用率作为重点的调度原则，因为移动端需要考虑散热问题，所以更好的平衡使用率和散热是移动端操作系统的主要原则

具体的评价指标有：

1. CPU 使用率：CPU 处于忙状态所占时间的百分比
2. 吞吐量：在单位时间内完成的进程数量
3. 周转时间：一个进程从初始化到结束，包含所有等待时间的总和，大致分为等待时间和服务时间
4. 等待时间：进程在就绪队列中的总时间，是周转时间的一部分
5. 相应时间：从一个请求被提交到产生第一次响应所花费的总时间

此外，要延伸一个小的概念点，我们一直说什么什么系统快，而这里的快其实是通过两个指标来描述的，即：**低延迟**和**高带宽**。这两个指标是独立存在的，并且有可能是成反比的一个高另一个就低，为此找到一个平衡点是很重要的。我之前一直分不清这两个指标的具体含义，通过以下的例子可以很好的描述它们。

拿水管举例：

1. 低延迟：喝水的时候一打开水龙头就有水流出
2. 高带宽：给游泳池充水时希望从水龙头里同时流出大量的水

而调度算法的设计仅仅结合实际场景尽可能的满足相应的指标，上述的所有指标都满足时不现实的。比如：桌面操作系统强调交互性高，因此需要尽可能地满足低延迟；而数据中心需要处理大量的数据，需要尽可能的高带宽。linux 在针对桌面程序的调度和针对 server 程序的调度算法是不相同的。

综上，调度算法的主要目标是：

1. 减少响应时间：及时处理用户的输出并且尽快将输出提供给用户
2. 减少平均响应时间的波动：在交互系统中，可预测性 比 高差异低平均 更重要
3. 增加吞吐量：两个方面：1) 减少开销（即上下文切换）；2) 系统资源的高效利用
4. 减少等待时间：减少每个进程的等待时间
5. 公平性：即对每个进程提供公平的调度时间，也有基于公平性实现的调度算法

上面具体说明了进程调度算法的目的，下面来具体看看都有哪些调度算法。

### FCFS

FCFS 即 First Come, First Served 先来先服务。这个算法其实和内存管理中的先进先出算法基本相同，都是所有算法中最简单的入门算法，其原理也很简单：将所有的进程置入队列排队，哪个先来，哪个就先执行结束。其周转时间如下图所示：

![time-fcfs](https://image.dunbreak.cn/os-process/time-fcfs.png)

可以看到，FCFS 算法的周转时间依赖于进程的执行顺序，顺序不同平均周转时间不同。

和内存管理一样，该算法的优点就是简单。而缺点主要有：

1. 平均等待时间波动较大
2. 花费时间少的任务可能排在花费时间长的任务后面
3. 可能导致 IO 和 CPU 之间的重叠处理：CPU 密集型进程会导致 IO 设备闲置时 IO 密集型进程也在等待

具体总结如下：

![summary-fcfs](https://image.dunbreak.cn/os-process/summary-fcfs.png)

### SPN(SJF) SRT

SPN(SJF) SRT 其实说的是三个算法，Shortest Porcess Next(Shortest Job First) Shortest Remaining Time，短进程优先（短作业优先）短剩余时间优先。其算法逻辑基本都相同，就是执行时间短的进程优先执行。具体的算法实现如下图所示：

![implement-spn](https://image.dunbreak.cn/os-process/implement-spn.png)

SJF 和 SPN 都是非抢占式的短进程优先算法，即如果在调度过程中，有一个进程的执行时间比正在执行状态中的进程还短，它不会打断当前进程而是将新进程置于任务队列顶部。SRT 则相反，使用抢占式调度，会将当前正在执行的程序置于队列顶部调用新进程。

此算法的优点是平均等待时间和平均周转时间最短，如下图：

![time-spn](https://image.dunbreak.cn/os-process/time-spn.png)

基于短任务优先的算法平均响应时间就是 r1 + .. + r6 / 6，如果改变了顺序，其平均响应时间的计算会发生改变，数学上可以证明其肯定大于第一种情况。

当然，此算法也有很明显的缺点：

1. 可能导致饥饿：连续的短任务流会使长任务饥饿；短任务可用时的任何长任务的 CPU 时间都会增加平均等待时间
2. 需要预知执行时间：这是比较困难的，只有实时系统中的才能知道执行时间。简单的解决方法就是询问用户，如果用户提供的值有误则直接杀死进程。

关于`预知执行时间`，其实后续的算法也同样需要此功能，目前这个问题的解决方案都是通过过去的执行时间预估未来，即如下算法：

![estimate-time](https://image.dunbreak.cn/os-process/estimate-time.png)

其算法的逻辑是：利用当前时刻的执行时间和当前预估的执行时间计算出下一个时刻的 CPU 使用率。其中 τn+1 表示下一个时刻的执行时间，α 表示系数，tn 表示当前时刻的执行时间。

具体 SPN 算法总结如下：

![summary-spn](https://image.dunbreak.cn/os-process/summary-spn.png)

### HRRN

HRRN 是 Highest Response Ratio Next 高响应比优先。此算法是在 SPN 的基础上改进的，主要关注进程等待了多长时间，防止无限期推迟，是不可抢占的算法。算法依据是：

```bash
R = (w+s)/s

# w：waiting time 等待时间
# s：service time 执行时间
# 算法选择 R 值最高的进程
```

其算法的优点是：充分考虑了等待时间，因此饥饿现象不会产生；缺点是：不支持抢占式调度，并且同样需要预估未来的执行时间。具体总结如下：

![summary-hrrn](https://image.dunbreak.cn/os-process/summary-hrrn.png)

### Round Robin

Round Robin 即时间片轮转，其逻辑是设置好一个时间片，让等待调度的进程依次执行。

此算法的优点就是公平；而缺点是：平均等待时间很大，如下图所示：

![example-rr](https://image.dunbreak.cn/os-process/example-rr.png)

从上面的例子可以看到，时间片轮转虽然公平，但是平均响应时间大部分时候都长于 FCFS 算法，可以看到想要达到公平性也是需要付出一些代价的。

除此之外，时间片的设置也很重要。如果时间片设置的较小，可能会将性能消耗在的上下文切换中；如果时间片设置的较大，那会退化成 FCFS 算法。目前通过经验，早期 Linux 的时间片都是 1/100 秒，现在 Linux 的时间片一般是 1/1000 秒。

具体总结如下：

![summary-rr](https://image.dunbreak.cn/os-process/summary-rr.png)

### Multilevel Feedback Queues

即多级反馈队列。首先需要看一下多级队列算法：该算法是将上述的一些算法进行综合，可以把进程按照优先级分为多个队列，每个队列的调度算法不相同。优先级最高的队列可以用 FCFS 算法。但是，由于进程是动态性的，在执行过程中可能不同的阶段需要调度的类型是不同的会变化。

为此出现了多级反馈队列：

![implement-mfq](https://image.dunbreak.cn/os-process/implement-mfq.png)

简单来说就是：如果有任务执行的时间越长，那么它的优先级越低，如果一个任务的等待时间越长，那么它的优先级越高。并且 **IO 密集型**的交互性任务优先级**高于** **CPU 密集型**的计算类任务。

具体总结如下：

![summary-mfq](https://image.dunbreak.cn/os-process/summary-mfq.png)

### Fair Share Scheduling

即公平共享调度。上述的几个算法虽然或多或少的考虑公平性，但是并没有把公平性当成第一个要素考虑，而公平共享调度就是针对公平性设计的调度算法。

这种算法出现的主要原因是早期或者近期的操作系统都是多用户的，多个人共享一个操作系统，因此为每个用户提供相同的公平性就是主要的职责。也就是说，该算法是基于用户级别而不是进程级别提供一个公平共享的调度。比如目前的 Linux 调度算法就是一种公平的调度算法 [Completely Fair Scheduler](https://en.wikipedia.org/wiki/Completely_Fair_Scheduler)。

### 实时调度及异常现象

上述的都是一些通用的调度算法，与之对应的还有实时调度算法，本节主要了解一下实时调度算法以及一个异常现象。

#### 实时调度

实时调度面向的使用者不同，一般都是面向工业级别的火车控制，机床等实时性非常强的场景中。这类算法的核心是实时性，而相对的速度和平均性能相对就不是很重要。

实时系统分为硬实时和软实时。硬实时比如火车控制，水坝控制等如果达不到就会有灾难性后果的场景中；而软实时比如播放电影需要尽可能的达到 60 帧每秒，但是如果没有达到没有什么大问题。

实时调度算法只要分两类，如下图所示只做大致了解即可：

![summary-realtime](https://image.dunbreak.cn/os-process/summary-realtime.png)

#### 多处理器调度

目前的手机电脑等多事多核系统，因此多处理器调度与上述的通用型调度算法不同需要考虑多个核心利用率的问题。想要达到的目标是负载均衡，即尽量不能让一个 CPU 非常忙而一个 CPU 很闲。此类的调度算法在一个 CPU 内就和上述的通用调度算法基本相同，不同是此类算法需要考虑 CPU 动态利用率的问题。

#### 优先级反转

优先级反转主要和临界区资源有关，比如有下述三个进程极其优先级为：T1 > T2 > T3 如下图所示：

![priority-inversion](https://image.dunbreak.cn/os-process/priority-inversion.png)

系统最开始时 T3 开始执行，执行过程中，T1 准备就绪进入执行队列开始执行，但是由于 T1 需要访问了被 T3 占用的临界区资源，因此 T1 被阻塞了需要等到 T3 释放临界区才能继续调度，但是返回 T3 继续执行时 T2 进入了执行队列，就导致 CPU 现在去处理 T2。

最终的结果就是，虽然 T1 的优先级高于 T2，但是在实际系统中，T2 优先于 T1 执行结束。此问题的解决方法是：

1. 优先级继承，即如果 T1 访问了和 T3 相同的临界区，那么会将 T3 的优先级提高到和 T1 相同，这样 T2 就不会抢占 T3
2. 优先级天花板协议：即资源的优先级和所有可以锁定该资源的任务中优先级最高的那个任务的优先级相同。此时，除非优先级高于系统中所有被锁定的资源优先级上限，否则任务尝试执行临界区是就会被阻塞。并且，持有最高优先级上限信号量锁的任务，会继承被该锁锁阻塞任务的优先级

## 同步互斥

除了调度算法，操作系统进程管理的另一大模块就是对同步互斥的处理。

根据上一节的优先级反转的情况我们可以知道，进程的调度其实还依赖于进程是否需要访问**共享资源**，如果访问共享资源，那该进程具有`不确定性`和`不可重复性`，因为该进程的最终调度需要和访问共享资源的其他进程一起决定。这种不确定性和不可重复被统一称为`竞态资源`。

### 为什么会出现互斥？

下面有一个例子可以很好的说明为什么会出现互斥的概念：

我们知道，fork 进程的时候需要返回一个 pid，而每次的 pid 都是需要增加的，如下的 c 语言代码就是增加 pid 的值的：

```cpp
/*注意这两个变量不相同，一个是new，一个是next*/
new_pid = next_pid++;
```

但是，该语句被编译成汇编之后则成为一下四条语句：

```assembly
LOAD next_pid Reg1
STORE Reg1 new_pid
INC Reg1
STORE Reg1 next_pid
```

在单一进程中，上述的代码是没有问题。但是如果此时存在并发性，同时有两个进程一起 fork 进程，那可能会出现以下的情况，假设刚开始 next_pid=100：

![assembly](https://image.dunbreak.cn/os-process/assembly.png)

按照正常来说，进程 1 获得的 pid 应该等于 100，而进程二获得的 pid 应该等于 101，并且此时的 next_pid 应该==102 才对。但是由于进程一执行过程中进程二被调度，从而使得两个进程获得了相同的 pid。

出现上述问题的原因是 `new_pid=next_pid++;`操作不是`原子性`的，可以在执行过程中被打断。一个生活中的例子可以更形象的描述这个问题：

![buy-bread](https://image.dunbreak.cn/os-process/buy-bread.png)

### 临界区

为了解决这种问题，出现了以下两种限制：

- 临界区：Critical Section，临界区是指进程中的一段共享资源的代码区域，并且当一个进程处于相应区域代码段时，其他所有进程都不能执行的代码区域。共享资源相应的代码段
- 互斥：Mutual Exclusion，当一个进程处于临界区并且访问共享资源时，没有其他进程会区域临界区并且访问任何相同的共享资源。确保只有一个进程在临界区被称为互斥

为了实现临界区互斥，一般有如下四个条件，如果满足了下述的条件，则表示可以满足临界区资源的互斥性，主要有：

1. 互斥
2. Progress 前进：即进程访问临界区之后可以继续前进
3. 优先等待：可以算是对 Progress 的进一步说明，表示等待进入临界区的线程不能无限等待
4. 避免忙等（可选）：可知自旋锁是白白消耗 CPU 算力，因此应该尽可能的避免忙等。此条件可选

### 互斥的实现

早期可以使用硬件和软件方式实现。

#### 硬件实现

最早的硬件方式是`禁用中断`。

虽然取消中断确实可以解决临界区资源互斥的问题，但是从而引入了新的问题：_程序无法响应 IO 中断事件_。并且，在多核 CPU 下，一个 CPU 核心无法中断，其余的 CPU 依然可以并行访问临界区资源。

因此，早起的硬件方式不是一个很好解决临界区资源互斥的方法。

#### 软件实现

软件方法同样可以解决临界区资源互斥的问题，但是直到 1981 年，才由 Peterson 完成。我们可以看一下这种算法是怎样一步一步实现的。

假如目前有两个线程，编号分别是 _Pi=0_, _Pj=1_ 同时想要访问临界区的资源，则临界区互斥软件实现算法如下。

##### 版本一，使用一个变量

```cpp
int turn = 0;
/*表示i线程想要进入临界区，同理，如果当前是j线程想要进入临界区，则此处应该赋值turn = j*/
turn = i;
do {
    while(turn != i) ;
    critical section
    /*将turn标记为另一个线程的id表示另一个线程可以进入临界区*/
    turn = j;
    reminder section
} while(1);
```

上述算法中确实可以满足互斥逻辑，但是满足不了*前进 Progress*逻辑。

因此加入线程 i 执行结束之后在执行`turn = j;`之后退出了该临界区代码，则如果线程 j 想要继续访问临界区，则会一直卡死在`while (turn != j) ;`这里。因此这种算法只能满足于 i 和 j 两个进程轮流交替访问临界区资源的目的，如果一个退出了另一个想要继续访问就会被死锁。

##### 版本二，使用一个数组

```cpp
int flag[2];
flag[0] = flag[1] = 0;
do {
    /*同上一个版本，如果线程i想要进入临界区则判断另一个线程j是否为1，如果为1表示线程j正在访问临界区资源*/
    while(flag[j] == 1) ;
    flag[i] = 1;
    critical section
    flag[i] = 0;
    remainder section
} while(1);
```

上述算法看似没啥问题，可以满足前进问题，但是没有满足*互斥*。

因为如果线程 i 执行了`while(flag[j] == 1) ;`之后还没来得及执行`flag[i] = 1;`就发生了上下文切换到线程 j，则 i 和 j 就可以同事访问临界区资源。

##### 版本三，对版本二增强

```cpp
int flag[2];
flag[0] = flag[1] = 0;
do {
    /*同上一个版本，此版本就是将锁定临界区赋值的语句提前到了自旋锁外*/
    flag[i] = 1;
    while(flag[j] == 1) ;
    critical section
    flag[i] = 0;
    remainder section
} while(1);
```

这个版本虽然满足了互斥和前进的逻辑，但是有可能*产生死锁*。

如果当线程 i 执行`flag[i] = 1;`之后切换到线程 j 的话，那么就有可能同时存在`flag[i] == flag[j] == 1`的情况，那这时两个线程就都会死锁在`while(flag[j] == 1) ;`这里。

##### Peterson 算法

```cpp
int turn = 0;
int flag[2];
flag[0] = flag[1] = 0;
do {
    flag[i] = 1;
    turn = j;
    while(flag[j] == 1 && turn == j) ;
    critical section
    flag[i] = 0;
    remainder section
} while(1);
```

可以看到，Peterson 算法就是将上述的三个版本的逻辑想结合，使用了两组变量 turn 和 flag 分别表示另一个线程是否在临界区内，从而实现了两个线程的临界区互斥逻辑。

以上才是两个线程共同访问临界区的互斥问题，此问题在 1981 年才被解决。而多个线程访问临界区资源互斥的软件实现，是`Eisenberg and McGuire's Algorithm`，是基于循环队列的方式实现的，以及`Bakery算法`基于临界区标志和 pid 编号实现。这两种算法过于复杂就不深入学习了。

### 目前的实现

综上，不管是纯硬件还是纯软件方式实现临界区资源互斥或多或少都存在问题，硬件方式无法响应中断，软件方式过于复杂。

为此，更高级别的计算机体系结构视图解决这种问题，也有进入了两个计算机系统中的两个原子性指令：`test-and-set`和`exchange`这两条指令只要有一条提供了，就可以实现临界区的互斥逻辑。目前大部分的互斥逻辑都是通过这两个原子性指令实现的。

- test-and-set：从内存中读取值，测试该值是否为 1，然后返回真或假，返回的同时将内存值设置为 1
- exchange：交换内存中的两个值

#### test-and-set 实现互斥

![test-and-set](https://image.dunbreak.cn/os-process/test-and-set.png)

算法逻辑是：

1. 初始时 value == 0
2. 当有现成想要进入临界区获取锁时，调用 test-and-set 指令
3. 由于此时 value == 0，所以 test-and-set 返回 false 并不会自旋，并且把 value 赋值为 1
4. 当其他线程获取锁时，自旋
5. 当第一个线程执行结束后，将 value 赋值为 0 释放锁，则后续的线程就可以访问临界区了

可以看到如果使用 test-and-set 方法实现互斥则非常简单，并且不论是一个线程还是多个线程，其进入临界区的逻辑都是一致的。

不过上述的代码还是有可以改进的地方，就是获取锁的地方使用了**自旋锁**，如果临界区很长，则会白白消耗 CPU 算力。

结合上述的线程管理的内容，一个好的解决方法是将线程的 TCB 加入到等待队列中，在释放锁时再从等待队列中拿出一个线程唤醒，如下图所示：

![enhanced-test-and-set](https://image.dunbreak.cn/os-process/enhanced-test-and-set.png)

其实，如果临界区很短的情况下，自旋其实是一个很好的选择，因为不用进行线程上下文切换。

#### exchange 实现互斥

```cpp
int lock = 0;
int key = 0;
do {
    key = 1;
    while(key == 1) {
        exchange(lock, key);
    }
    critical section
    lock = 0;
    remainder section
} while(1);
```

此算法的逻辑是：

1. 初始时，lock 和 key 都是 0
2. 当有一个线程想要访问临界区时，会把 key 赋值为 1，从而进入自旋判断，此时 exchange 指令会把 key 赋值为 0，lock 赋值为 1
3. 因为 key 已经 等于 0 了，此时第一个线程进入临界区
4. 当有其他进程想要进入临界区时，首先会把 key 又赋值为 1，则此时会自旋在 exchange 指令中，因为此时 lock 和 key 都是 1 了
5. 当第一个线程执行结束后，将 lock 赋值为 0，则后续的线程可以继续访问临界区

综上，基于原子性操作的方式实现临界区互斥的功能很简单，并且很容易扩展到多个线程，但是也有如下的几个问题：

1. 程序中使用自旋逻辑让 CPU 忙等消耗 CPU 处理时间
2. 由于获取锁的逻辑是一个竞争的逻辑，所以在多线程的情况下有可能出现有一个线程始终抢不到锁从而饥饿的情况
3. 在实时性系统中可能存在死锁，死锁的情况和上一节讲过的优先级反转的情况一样，当一个低优先级的进程抢到了锁而正好高优先级进程又访问临界区时，高优先级在忙等而低优先级无法释放锁从而死锁

### 信号量

上一节讲到的所有关于互斥的逻辑都是基于多线程只有一个线程访问临界区资源的情况。但是大部分时候可能临界区的资源是可以让多个线程读而只有一个线程写这种情况，而同时让多个线程读取临界区资源的技术可以通过信号量实现。

信号量 semaphore，其意思和日常生活中见到的信号灯（红绿灯）类似。一般信号量由一个有符号整型表示，有两个操作，**P()**和 **V()**。其操作必须都是**原子性**的，并且一般使用时，信号量的初始值都是大于 0 的。

1. P()：sem 减 1，如果 sem < 0 则等待，否则继续
2. V()：sem 加 1，如果 sem<= 0，则唤醒一个等待的程序

> 信号量是计算机祖师爷荷兰计算机科学家 _Dijkstra_ 在 20 世纪 60 年代提出的，其中 P 和 V 其实是 Prolaag 和 Verhoog 其实是荷兰语中的减少和增加。

上一节说到的临界区互斥锁其实可以通过一个二进制信号量实现:

```cpp
Semaphore mutex = new Semaphore(1);
...
mutex -> P();
...
critical section
...
mutex -> V();
remainder section
```

除了上述的互斥，利用信号量也可以实现同步的逻辑。比如如下的代码是使用信号量实现线程 A 和线程 B 的同步逻辑，线程 A 在执行过程中需要等待线程 B 的结果：

```cpp
Semaphore condition = new Semaphore(0);
...
// Thread A
condition -> P();
...


// Thread B
...
condition -> V();
```

可以看到，如果要实现同步逻辑，信号量的初始值就`必须是0`，其主要逻辑是：

1. 初始时将信号量赋值为 0
2. 线程 A 执行过程中，如果需要同步线程 B，则需要调用 P()操作，此时线程 A 进入等待队列中
3. 线程 B 执行结束后，调用 V()释放信号量
4. 此时线程 A 被唤醒继续执行，从而达到同步的逻辑

#### 实现原理

信号量可以通过禁用中断或者 test-and-set 实现，其实现原理类似于锁。比如如下是一个使用禁用中断实现的一个信号量代码：

```cpp
// 初始化
int sem;
WaitQueue q;

P() {
    sem--;
    if (sem < 0) {
        add this thread t to q;
        block(t);
    }
}

V() {
    sem++;
    if (sem <= 0) {
        remove a thread t from q;
        wakeup(t);
    }
}
```

可以看到实现起来还是很简单的，不过需要注意的是 V 操作下的判断条件是`if (sem <= 0)`，并且信号量内部都是使用等待队列实现等待的和锁不同。

#### 生产者消费者问题

生产者消费者问题是同步互斥的经典问题之一，是说：有 N 个生产者同时可以向 buffer 中写入数据，并且同时也可以有 M 个消费者从 buffer 中消费数据。此时会存在如下的几种情况：

1. 当 buffer 为空，则消费者应该被挂起，直到生产者想 buffer 中写入了数据，才应该唤醒
2. 当 buffer 满了时，生产者应该被挂起，直到消费者消费了数据使得 buffer 不满，才应该唤醒一个生产者

则上述的模型可以使用如下的代码实现：

```cpp
// 初始化
// 由于对buffer的操作同一个时间只能有一个，因此需要一个互斥锁保证访问buffer的线程互斥
mutex = new Semaphore(1);
// 刚开始时buffer的内容是空的，因此fullBuffers这个信号量在起始时是0表示当前buffer中没有数据
fullBuffers = new Semaphore(0);
// num = buffer.size()，初始时，设置emptyBuffers为buffer的size大小，表示当前的buffer中还可以写入的数据
emptyBuffers = new Semaphore(num);

// 生产者生产产品
Deposit(c) {
    // 首先调用emptyBuffers的P操作判断当前的buffer是否被写满，如果被写满则当前的生产者会被挂起
    emptyBuffers -> P();
    // 由于对buffer的操作需要互斥性，因此在操作buffer时要加上互斥锁
    mutex -> P();
    add c to buffer;
    mutex -> V();
    // 对buffer加完数据之后，要对fullBuffers进行V操作告诉消费者buffer中新增了一个产品
    fullBuffers -> V();
}

// 消费者消费产品
Remove() {
    // 同理，消费者消费产品时，首先应该判断buffer中是否还有剩余的产品
    // 如果产品为空，则当前的消费者需要被挂起
    fullBuffers -> P();
    // 同理操作buffer是一个互斥操作
    mutex -> P();
    remove a c from buffer;
    mutex -> V();
    // 当消费成功后，需要释放一个emptyBuffers，表示告诉生产者产生了一个空位
    emptyBuffers -> V();
}
```

综上，一个生产者-消费者的模型需要三个信号量完成：

1. mutex：一个基准的互斥信号量，因为操作 buffer 的操作要互斥
2. emptyBuffers：生产者关注的信号量，起始值应该是 buffer 的 size，表示告诉生产者 buffer 中还有多少个空位置
3. fullBuffers：消费者关注的信号量，初始值为 0 因为刚开始 buffer 中没有数据可以消费，表示告诉消费者当前的 buffer 中已经存在多少个产品可以被消费

### 管程

虽然信号量很灵活，但是使用起来非常困难，很容易出现错误，比如使用信号量已经被另一个线程占用，忘记释放信号量等等，如果使用错误可能后果比较严重，此外信号量也无法解决死锁问题。

为此，为了更好的使用锁的机制，出现了管程技术。

**管程 Monitor** 是比信号量*更高级的抽象*概念，其目的是为了分离互斥和条件同步的关注。这个概念不是操作系统级别的而是**语言级别**的，因此更方便程序员使用。

什么是管程？管程是包含了一系列的共享变量，以及针对这些变量操作的函数的组合。是一个概念，其其实现需要至少两个模块：

1. 一个锁：指定临界区
2. 0 个或多个条件变量：等待/通知信号量用于管理并发访问共享数据

![monitor](https://image.dunbreak.cn/os-process/monitor.png)

从上图可以看到一个管程的主要使用流程，主要如下：

1. 使用管程首先就需要进入一个 entry queue 等待锁的释放，同一时刻只有一个程序能够访问管程内的资源
2. 当获得了锁后，对应的进程就获得了管程内的所有函数的使用权即上图的 operations
3. 但如果调用的某个函数需要访问的共享资源无法满足了，那么就需要互斥地等待，并且需要把自身挂到某个等待队列中去。管程中的等待队列就是通过条件变量实现的，上图中就维护了两个条件变量队列 x 和 y。并且等待时需要把自己的 lock 释放掉，以供其他的程序继续访问管程。这个等待的逻辑是由条件变量调用 wait 函数实现的
4. 当条件变量满足时条件变量调用 signal 函数唤醒之前的程序，重新获得锁进行对共享数据的操作

条件变量的实现代码如下：

```cpp
// 初始化
int numWaiting = 0;
WaitQueue q;

// 等待操作
Wait(lock) {
    numWaiting++;
    add this thread t to q;
    release(lock);
    schedule(); // need mutex
    require(lock);
}
// 唤醒操作
Signal() {
    if (numWaiting > 0) {
        remove a thread t from q;
        wakeup(t); // need mutex
        numWaiting--;
    }
}
```

可以看到每个条件变量都需要维护自己的队列，并且最主要的， wait 函数内先调用了`release(lock);`命令，这是因为管程的定义确定了在管程内执行的程序应该先获的一个互斥锁，这里释放的就是外面获取到的互斥锁，以供管程外的其他程序可以访问管程。

下面是使用管程实现的一个生产者消费者问题：

```cpp
// 初始化
Lock lock;
int count = 0;
Condition notFull, notEmpty;

// 生产产品
Deposit(c) {
    lock -> Acquire();
    // 这里用while不使用if是因为Remove函数并没有判断count的数量，因此有可能出现多个count == num的情况，因此需要使用while将其全部wait
    while (count == num) {
        // num就是buffer的size
        notFull.Wait(&lock);
    }
    add c to the buffer;
    count++;
    notEmpty.Signal();
    lock -> Release();
}

// 消费产品
Remove() {
    lock -> Acquire();
    while (count == 0) {
        notEmpty.Wait(&lock);
    }
    remove a c from the buffer;
    count--;
    notFull.Signal();
    lock -> Release();
}
```

### 读者-写者问题

读者-写者同样也是比较经典的同步互斥问题之一，说的是：现在有一个共享的数据资源，同时有 N 个读者需要读取资源，并且有 M 个写者需要写该资源。为此如何设计同步互斥机制保证读写能够顺利完成并且保证实现如下的约束：

1. 并且有如下的条件：多个读者可以同时读取数据
2. 多个写者不能同时写数据
3. 写者必须独占竞态资源

基于上述的条件，此问题还可以继续细化为`读者优先`和`写者优先`。

#### 读者优先

下面是一个使用`信号量`实现的代码：

```cpp
// 初始化
mutex = new Semaphore(N);
writerMutex = new Semaphore(1);
int Rcount = 0;

// 写者操作
Write() {
    writerMutex -> P();
    do write;
    writerMutex -> V();
}

// 读者操作
Read() {
    mutex -> P();
    if (Rcount == 0) {
        writerMutex -> P();
    }
    Rcount++;
    mutex -> V();
    do read;
    mutex -> P();
    Rcount--;
    if (Rcount == 0) {
        writerMutex -> V();
    }
    mutex -> V();
}
```

#### 写者优先

写着优先的逻辑使用`管程`实现：

```cpp
// 初始化
int activeReaders = 0;
int activeWriters = 0;
int waitingReaders = 0;
int waitingWriters = 0;

Condition okToRead;
COndition okToWrite;
Lock lock;

// 读者操作
Read() {
    StartRead();
    do read;
    DoneRead();
}
StartRead() {
    lock -> Acquire();
    while ((activeWriters + waitingWriters) > 0) {
        waitingReaders++;
        okToRead.Wait(&lock);
        waitingReaders--;
    }
    activeReaders++;
    lock -> Release();
}
DoneRead() {
    lock -> Acquire();
    activeReaders--;
    if (activeReaders == 0 && waitingWriters > 0) {
        oktoWrite.Signal();
    }
    lock -> Release();
}

// 写者操作
Write() {
    StartWrite();
    do write;
    DoneWrite();
}
StartWrite() {
    lock -> Acquire();
    while ((activeWriters + activeReaders) > 0) {
        waitingWriters++;
        okToWrite.Wait(&lock);
        waitingWriters--;
    }
    activeWriters++;
    lock -> Release();
}
DoneWrite() {
    lock -> Acquire();
    activeWriters--;
    if (waitingWriters > 0) {
        oktoWrite.Signal();
    } else if (waitingReaders > 0) {
        // 需要广播将所有待读者全部释放
        okToRead.Broadcast();
    }
    lock -> Release();
}
```

### 哲学家进餐问题

如果说上面的问题仅是同步互斥的经典问题之一，那此问题应该是同步互斥最经典的问题了。具体描述如下：

![philosopher-question](https://image.dunbreak.cn/os-process/philosopher-question.png)

> 可以看到这是一个 Dijkstra 老爷子提出的一个问题。

这个问题看起挺简单的啊，为什么说是最经典的呢？我们应该可以同样使用上面所学的信号量或者管程，把叉子作为竞态资源进行互斥不就可以了吗？比如下面的实现。

#### 版本一

```cpp
while (1) {
    // 哲学家在思考
    think();
    // 拿起左边的叉子
    take_fork(i);
    // 拿起右边的叉子
    take_fork((i + 1) % N);
    eat();
    put_fork(i);
    put_fork((i + 1) % N);
}
```

上述代码对一个哲学家确实没有问题，但是在多个线程同时访问时很可能*导致死锁*。

比如现在 5 个线程同时访问到`take_fork(i);`后，就会出现所有的叉子都拿完了，但是另一个叉子所有人都拿不到的情况。

#### 版本二

那么自然而然地就会想到对上述问题进行一点修改，代码如下：

```cpp
    // 哲学家在思考
    think();
    // 开始拿叉子
    while (1) {
        // 拿起左边的叉子
        take_fork(i);
        if (fork[(i + 1) % N]) {
            take_fork((i + 1) % N);
            break;
        } else {
            put_fork(i);
            wait_some_time();
        }
    }
    // 说明都拿到了，开始吃饭
    eat();
    // 吃完释放叉子
    put_fork(i);
    put_fork((i + 1) % N);
}
```

这段代码对上述进行了一些改进，当一个哲学家如果拿不到另一个叉子时，会主动释放自己的叉子让其他哲学家先吃，等他们吃完再拿。代码看上去没有问题，但是其实还是存在所以哲学家都*饥饿*的情况。

比如 5 个线程同时执行了 while 语句中的`take_fork(i);`，之后发现另一个叉子没有又同时执行了`put_fork(i);`就导致所有的线程都在重复执行相同的操作，导致线程饥饿。

当然，上述的问题虽然可以通过修改`wait_some_time();`为`wait_random_time();`来实现，将等待特定的时间改为等待随机时间，使得每个哲学家等待的时间不相同，则不会出现上面的问题。但是这种改法不够完美，因为加入了随机数会存在不确定性，还是会有极低的可能存在所有线程饥饿的情况。

#### 版本三

继续改进，通过上面学到的互斥的概念，我们可以对拿叉子的操作加入互斥锁，代码如下：

```cpp
while (1) {
    // 哲学家在思考
    think();
    // 进入临界区
    mutex -> P();
    // 拿起左边的叉子
    take_fork(i);
    // 拿起右边的叉子
    take_fork((i + 1) % N);
    eat();
    put_fork(i);
    put_fork((i + 1) % N);
    // 退出临界区
    mutex -> V();
}
```

这种改法可以完美解决死锁问题，但是*降低了系统性能*。

因为每次只能有一个哲学家进餐，但是叉子明明有 5 支至少可以保证两个哲学家进餐。

#### 正解

为了设计更好的哲学家进餐算法，应该对这个问题进行进一步的抽象，整个思考逻辑如下：

1. 哲学家思考
2. 饥饿需要吃饭，此时应该看一下他两边的哲学家的状态
3. 如果左右两边有一个哲学家在吃饭，那说明此时他自己没办法吃饭，只能等待
4. 如果左右两边都不在吃饭，则此时说明哲学家可以吃饭，那么拿起两个叉子开始吃饭
5. 吃完饭之后他还可以判断一下左右两边哲学家的状态，如果有哲学家处在饥饿状态，那么应该去唤醒他让他尝试去吃饭

基于上面一种思考，我们可以看到之前的代码都是把叉子作为竟态资源考虑的，其实哲学家的状态也可以封装为竟态资源，只有当一个哲学家两侧的哲学家都没有在吃饭时，当前的哲学家才能吃饭。为此，最终的哲学家进餐问题的代码如下：

```cpp
// 初始化
// 哲学家人数
#define N 5
// 第i个哲学家的左邻居
#define LEFT (i + N - 1) % N
// 第i个哲学家的右邻居
#define RIGHT (i + 1) % N
// 思考状态
#define THINKING 0
// 饥饿状态
#define HUNGRY 1
// 进餐状态
#define EATING 2

// 记录每个人的状态
int state[N];
// 整体互斥信号量，初始值为1
Semaphore mutex;
// 同步信号量，初始值为0
Semaphore s[N];

void philosopher(int i) {
    while (1) {
        think();
        taske_forks(i);
        eat();
        put_forks(i);
    }
}

void task_forks(int i) {
    mutex -> P();
    state[i] = HUNGRY;
    // 尝试拿左右两边的叉子
    try_take_left_right_forks(i);
    mutex -> V();
    // 这里将自己的同步信号量减一，因为如果上述尝试成功，则需要还原信号量，如果上述尝试未成功，则需要将自己加入等待状态
    s[i] -> P();
}

void try_take_left_right_forks(int i) {
    if (state[i] == HUNGRY && state[LEFT] != EATING && state[RIGHT] != EATING) {
        state[i] = EATING;
        // 表示通知编号为i的哲学家吃饭
        // 对应task_forks函数中最后的s[i] -> P();
        // 如果下面的函数执行，则上述操作为还原，如果未执行，则上述操作是将自己置入等待队列等待周围的哲学家唤醒
        s[i] -> V();
    }
}

void put_forks(int i) {
    mutex -> P();
    // 交出两把叉子
    state[i] = THINKING;
    // 看左右邻居能否进餐
    try_take_left_right_forks(LEFT);
    try_take_left_right_forks(RIGHT);
    mutex -> V();
}
```

> 只能说，泰斗不愧是泰斗。这思考问题的方式和维度确实比我们这种码农厉害。

最后需要特别注意的是，由于状态是竞态资源，所以 think()函数在最开始调用时应该使用互斥信号量包裹起来将状态置为 THINKING。

### 死锁

死锁就是一个可以共享的资源被占用没有释放最终导致饥饿的一种现象。

其中的共享资源可以是物理层面的 CPU 周期，IO 设备等，也可以是抽象层面的一个数据结构。当一个进程已经拥有一个资源之后再去请求使用别的资源，就有可能发生死锁。

死锁现象出现的必要条件有以下四个：

1. 互斥：在一个时间只能有一个进程使用资源
2. 持有并等待：进程持有至少一个资源正在等待获取其他进程持有的额外资源
3. 无抢占：一个资源只能被进程自愿释放
4. 循环等待：存在 P0-Pn 资源的循环等待

上述的条件是死锁出现的`必要条件`，也就是说如果出现死锁现象那么上述四个条件必定全部都满足。但是如果全部满足上述四个条件，则不一定产生死锁。满足上述四个条件的进程状态被称为`不安全状态`，即有可能产生死锁的状态。

为了更好地描述死锁现象，可以对死锁问题进行如下的建模。

定义目前需要访问共享资源的进程集合`{P0, P1, ..., Pn}`，以及共享资源类型的集合`{R0, R1, ..., Rn}`。则，如果一个进程 Pi 想要请求共享资源 Ri，那么其实可以表示为 Pi 指向 Ri 的一条有向的边 `Pi --> Ri`；如果一个资源 Ri 被进程 Pi 所独占，那么也可以表示为从 Ri 指向 Pi 的一条邮箱边`Ri --> Pi`。那么上述的两个集合如果全部以此方式表示，则可以形成一个有向图，如下图所示：

![deadlock-no-circle](https://image.dunbreak.cn/os-process/deadlock-no-circle.png)

通过这样的建模，我们可以很直观的在图中看到一个系统是否处于不安全状态，比如下图：

![deadlock-with-circle](https://image.dunbreak.cn/os-process/deadlock-with-circle.png)

由于图中出现了`P1 --> Ri --> P2 --> R3 --> P3 --> R2 --> P1`这样的环（其实还有另一个小环），那么说明系统中出现了循环等待，那么这个系统就是不安全的，有可能产生死锁。为什么说有可能，因为并不是有环就会产生死锁，比如下图：

![no-deadlock-with-circle](https://image.dunbreak.cn/os-process/no-deadlock-with-circle.png)

因此，有环图不一定会产生死锁，具体条件和资源的数量以及请求资源的进程数量相关。

#### 如何处理死锁？

通过上面的必要条件我们可以知道，只要是死锁必须会`同时满足`上述的那些条件，那么处理死锁的方法就是打破上述的四个必要条件中的一个即可。

- 如果我们尝试打破`互斥性`，那么根据之前所学的内容，共享资源没有互斥性会导致不确定性，从而引发进程不可预知的其他错误结果，所以`互斥`是不能打破的
- 如果我们想改变`占用并等待`，那么我们只能保证进程要么全部占有，那么全部不等待，那么这种处理方式会导致系统资源的利用率低的问题，比如哲学家算法中的版本三，将叉子整个互斥访问，虽然可以解决死锁问题，但是会导致同一时间只能有一个哲学家吃饭的结果
- 那如果打破`无抢占`的条件，让所有进程都可以被抢占，那进程调度系统就无法正常工作了，因为一个进程可以抢占正在持有共享资源并且运行的进程导致其无法正常运行结束

所以，处理死锁的最好方式，其实是打断`循环等待`问题。

为了更好的处理死锁问题，计算机科学家提出了一下四种避免或者解决死锁的方法：`死锁预防`、`死锁避免`、`死锁检测`以及`死锁恢复`。这四个方法的强度是逐级递减的。

#### 死锁预防

死锁预防一般应用于一些嵌入式的系统中，因为过于强硬会导致整个系统的利用率低。一般早起的 Unix 系统中使用的都是死锁避免的处理方式。

#### 死锁避免

死锁避免处理方式的核心依然是打破`循环等待`，一个好的解决方案是：找到一个安全序列`<P1, P2, ..., Pn>`使得序列中的进程按顺序访问竟态资源，则整个系统最终可以安全地运行。此方案的原理其实是：有的进程需要的资源多，有的进程需要的资源少，我们可以让需要资源少的进程先执行，结束后释放了自己的资源，那么需要资源多的进程也就可以执行了，因此会形成一个执行的顺序序列。

那么业界通用的死锁避免的排查算法就是`银行家算法`了。

##### 银行家算法

![banker's-algorithm](https://image.dunbreak.cn/os-process/banker's-algorithm.png)

> 可以看到这个算法同样是 Dijkstra 提出的（老爷子还挺腹黑的，上面黑哲学家，这里黑银行家（笑））

算法的逻辑其实就是上述打破循环等待的方法，就是只给能正常运行的进程分配足额的竞态资源，让其先执行，结束之后再把释放的资源给其他需要资源大的进程，从而最终验证当前系统是否处于`不安全状态`。

该算法所需的数据结构为：

- n = 进程数量， m = 资源类型数量。 通过 n 和 m，我们就可以形成一个矩阵，该矩阵就表示进程 i 所需要资源类型 j 的个数 k。
- Max（总需求量）：表示进程 Pi 最多请求资源类型 Rj 的个数为 k，即可表示为`Max[i, j] = k`
- Available（神域空限量）：如果 Available[j] = k，则表示有 k 个类型 Rj 的资源实例目前是可以使用的
- Allocation（已分配量）：如果 Allocation[i, j] = k，则表示 Pi 当前分配了 k 个 Rj 的资源
- Need（未来需要量）：如果 Need[i, j] = k，则表示 Pi 可能需要 k 个 Rj 的资源来完成任务

综上我们可以看到如下的关系：`Max[i, j] = Allocation[i, j] + Need[i, j]`。

同样也会有如下的伪代码：

```cpp
// 初始化
Work = Available
Finish[i] = false for i - 1, 2, ..., n

while (true) {
    // 算法运行时，找到如下条件的i：
    if Finish[i] == false && Need[i] <= Work {
        // 将Work分配给i
        Work = Work - Need[i]

        // 如果此时i已经执行完成
        Finish[i] = true
        // 释放所有的Allocation[i]
        Work = Work + Allocation[i]
        continue
    }

    // 如果此时所有的进程都是finish状态，则说明当前的系统是安全的，否则是不安全的
    if Finish[i] == true for i - 1, 2, ..., n {
        return safe
    }
    return unsafe
}
```

通过上述的算法调用，最终就可以看到当前的系统的是否是不安全状态。

#### 死锁检测

死锁检测指的是检查当前系统是否处在不安全状态，和上述的死锁避免不同，死锁避免是在进程执行前，验证即将运行的系统是否是安全的，而死锁检测则是对当前正在运行的系统进行检测。检测结束之后一般什么都不做。

此外，死锁检测算法的时间复杂度是 `O(m \* n^2)`，可以看到是非常慢的算法，所以一般也不在正常系统中使用它，而是在代码调试时使用其验证系统的安全性。

死锁检测的方式是检测正在运行的进程调用中是否存在环，即将最开始的进程资源图简化为进程图之后，其内是否存在环：

![deadlock-detection](https://image.dunbreak.cn/os-process/deadlock-detection.png)

如果有环，则说明存在死锁。

#### 死锁恢复

目前并没有很好的死锁恢复机制，能做的就是杀死死锁状态下的一个进程，但是同样是进程，杀死进程的依据是什么呢？

通常情况下会按照*进程的优先级*、_进程的执行时间_、*进程所占用的资源高低*判断是否被杀死，一般会把优先级低、执行时间长、资源占用率高的进程先杀死。不过通常的系统一般都是不处理，等待用户重启计算机解决死锁问题。

可以看到，运行状态的死锁很难处理，到目前为止依然是计算机科学界尚未研究结束的课题。

## 进程间通信

最后让我们看看进程间通信的一些概念。

进程间通信一般分为**直接通信**和**间接通信**，顾名思义，直接通信就是不通过其他中转两个进程直接进行通信连接，比如共享内存；间接通信一般都需要 kernel 的介入，一个进程将数据发送给 kernel，再由 kernel 转发给另一个进程，比如 pipe，消息队列等。

进程通信主要用一下几种方式实现：

1. 信号（Signal）
2. 管道
3. 消息队列
4. 共享内存

### 信号

这里的信号不是上一节讲到的信号量，而是 Signal，是一个类似于系统中断的软件级的中断机制。一般是由进程启动时向操作系统*注册几个信号的 Handler*，如果运行过程中操作系统收到了其他进程发送过来的信号，那么操作系统会直接执行 Handler 的内容。

这种类似于中断的实现机制是，操作系统接收到信号时，修改正在运行的进程的程序计数器指针，将下一个堆栈指向 Handler 所在的位置，执行结束之后和中断一行再恢复到之前的位置。

信号虽然处理和响应速度都很快并且没有什么额外的消耗，但是并不能传递数据，只能作为通知机制发送几个信号标识，如果对应的进程有相关标识的 Handler 则可以在 Handler 内处理相应的信息，基本类似于事件的机制。

### 管道

管道是父进程通过文件将一个子进程的输出重定向到一个 buffer 中，并且将这个 buffer 指向另一个进程的输入最终达到进程通信的效果。

比如 Linux 中：`ls | more`中间的`|`就表示一个管道。其内容的重定向由其父进程`shell`完成，两个子进程`ls`和`more`并不知道（或并不关注）最终的输入输出到底是什么。如下图所示：

![pipe](https://image.dunbreak.cn/os-process/pipe.png)

需要说明的是，管道的 buffer 是有限的在 Linux 下一般是 4kb，如果 buffer 写满了，那其实写入的进程会被阻塞。

管道虽然很灵活，但是必须要在兄弟进程之间才能通信，如果两个进程没有任何联系，那么无法使用管道通信。

并且，管道中的数据会传递 buffer 中，其格式是字节流的形式，消费端必须要把字节流解析成对应的信息才能使用。

最后，管道是一个*半双工*传递的系统，所谓半双工，就是虽然数据可以从左向右或者从右向左传递，但是同一时刻只能有一端传入，另一端读取。

### 消息队列

消息队列就是我们日常工作中了解的那个消息队列，其克服了管道的两个限制，通行两端不必是兄弟进程，并且传入的数据可以是结构化有意义的内容，并且可以类似生产者消费者模型一样多个进程写，多个进程读。

![message-queue](https://image.dunbreak.cn/os-process/message-queue.png)

### 共享内存

和上面的几种方式不同，共享内存是**直接通信方式**，其工作原理是将两个进程内的*部分逻辑页指向相同的页帧*，这样就达到了数据共享的目的。

由于是直接通信，因此这种方式是速度最快并且数据传输量最大的一种方式；但是需要考虑的是，共享内存资源时竟态资源，因此必须要考虑互斥访问以避免不确定性问题。

![shared-memory](https://image.dunbreak.cn/os-process/shared-memory.png)

最后，除了上面所有的，进程间通信还可以使用 `socket`，并且 socket 目前所有功能都是在 kernel 中实现的。但是 socket 主要内容都是在计算机网络课程中介绍的，因此操作系统中没有把其列入主要的进程间通信的手段。

以上就是操作系统进程管理的全部内容了。可以看到进程管理占据了操作系统课程最大的篇幅，内部的理论和原理纷乱庞杂，需要多次学习和理解。
