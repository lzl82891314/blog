---
title: 重学操作系统——内存管理
url_name: os-memory
date: 2021-09-25 11:24:07
categories:
  - 操作系统
tags:
  - 操作系统
  - 基础知识
---

内存管理操作系统的核心功能之一，主要管理用户程序从硬盘到内存的调度。这一部分学完之后，基本可以解答 [问题三](https://www.dunbreak.cn/2021/09/24/os-basics/#%E4%B8%89%E3%80%81%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%88%86%E9%A1%B5%E5%A4%A7%E5%B0%8F%E4%B8%BA%E5%95%A5%E8%83%BD%E8%A7%A3%E5%86%B3-Not-enough-quota-%E9%97%AE%E9%A2%98%EF%BC%9F) 了。

<!-- more -->

## Warning Up

首先，我们需要知道为什么操作系统需要内存管理单元？

计算机的存储结构是金字塔形的，从最低的外部存储到最顶的 L0 缓存依次访问速度是逐级递增的，但是其价格和容量也是逐级递减的。虽然硬盘具有持久数据的能力，但是其读盘速度远小于 CPU 的执行速度（CPU 的执行速度可以到纳秒级别，而就算是 SSD 也需要毫秒级别的读取能力）。

因此，所有存放在硬盘中的程序，都需要加载到内存这种访问速度比硬盘快很多的设备中供 CPU 运行。而程序从硬盘到内存的加载需要操作系统管理，这也就是内存管理的主要任务。

其次，加载数据到内存不就是简单的 IO 操作吗？为啥需要操作系统来处理？

一段程序在编写时并不知道它最终会被存放在哪个内存单元中，但是程序的运行也需要内存资源，比如 new 一个 array 是需要运行时分配内存空间的。那这又是怎么实现的呢？

其实，程序内部维护着一套从`0开始连续的逻辑地址`，要 new 一个 array 那从 0 开始连续分配就可以了。在程序运行时，操作系统会将这些逻辑地址进行映射，最终通过各种映射算法加载进内存中供 CPU 执行。

最后，地址映射不就是一个 Map 就完事了吗，有什么复杂的点需要操作系统来做呢？

简单的地址映射确实是通过映射表完成的，但是内存映射远比想象中的复杂的多，提高内存的利用率，使计算机中的程序能更好的运行才是内存管理的核心点。

## 蛮荒时期，连续内存分配

和操作系统的发展史一样，内存管理的发展也是一步一步变化的。

在蛮荒时期，操作系统是将`连续的`逻辑地址`直接映射`到内存中的，这种分配方式被称为`连续分配`。这样分配的好处是分配算法简单，就是上一节说的一个映射表就可以搞定，但是无脑的这样分配有一个问题会逐步出现，那就是`外碎片`。

> 外碎片就是进程和进程之间产生的随便，比如 ABC 三个进程在内存中，以 A-->B-->C 的顺序依次存放，当 B 运行结束后，D 被加载到内存中形成了 A-->D-->C。如果 B 需要 10K，而 D 需要 8K，那么剩余的 2K 就是内存外碎片。在连续分配时期，如果没有这么大的程序那这 2K 就会被浪费无法使用。

所以，如果程序中出现了大量的外碎片，那及时现在空闲的内存有 1G，那也有可能放不下 100M 的程序。为了更好地分配内存提高利用率，出现了以下几个针对连续内存分配的算法：

1. 首次优先：即操作系统在内存中找到第一个满足应用程序所需内存的内存块进行分配。这种算法十分简单，但是缺点是极容易产生大量的内存碎片
2. 最佳匹配：即找到最满足应用程序所需的内存地址进行分配。这种算法下就需要操作系统对空闲内存进行排序，找到空闲内存和所需内存差值最小的那段地址进行分配。其优点就是对小程序很有利，缺点是还会分出更小的可能无法操作的内存碎片
3. 最差匹配：和最佳匹配完全相反，即找到差值最大的内存地址块进行分配，其也需要操作系统对空闲内存进行排序，优点是对大程序有利，而缺点是首先拆分大块内存，可能会导致最终大程序无法运行

此外，除了这些分配算法以外，还有两个内存优化的方法可以减少外碎片：

1. 内存压缩：可以将存在碎片的内存进行优化，比如把后面的程序依次复制到碎片段，从而删除碎片。这种方案存在压缩的时机问题，只有在程序中断时才能压缩，并且还会产生很大的开销
2. 交换 Swap：即可以将空闲的程序交换到硬盘中，然后在交换会碎片段中

## 中世纪，分段式管理

可以看到在蛮荒时期，不管是分配算法还是优化方法都存在很严重的问题，前者会产生外部碎片，后者为了优化外部碎片会导致极大的性能开销，像是在填坑，并没有解决根本问题。

为了解决这些问题，出现了`离散分配方式`，即分段管理。

在分段管理中，操作系统可以将程序拆分成一个一个的段，以段的方式加载进内存，这样可以极大的改善大程序无法问题分配的问题，最主要的，分段管理的内存加载是离散的，比如：

一个程序可以被分为 ABC 三段，操作系统可以将 B 段分配到 0~2048 这个地址段，将 C 段分配到 4196~8,192 这个地址段，将 A 段分配到 9,216~10240 这个地址段。其三段之间可以不连续并且是分散的。

为了实现内存分段，就需要有一个物理内存到程序逻辑内存的映射，这个映射逻辑很复杂，不同的程序还不相同，使用软件实现开销会很大。因此，离散分配方式都需要硬件的支持才能完成。

分段映射一般是由两部分组成：`分段号+偏移量`。上面说了分段需要硬件配合，以 x86 为例，x86 CPU 中就会有专门的段寄存器和地址寄存器专门用于分别存储分段编号和地址偏移量。除此之外，想要知道应用程序所在的物理地址，一般还需要一个分段号 --> 内存编号的分段表，这个分段表一般是由操作系统在初始化阶段完成的。

假如 CPU 获取到的程序逻辑地址为：

```shell
0xabcd 123456
```

其中前半部分（不确定几位，不同的配置分段位可能不同）即 abcd 存放的就是分段号，而后面的 123456 存放的就是偏移量。此时，CPU 取出 abcd 通过 分段配置表查询得到内存编号，然后加上后面的偏移量就可以得到最终的物理地址块了。

重定位寄存器记录了 `逻辑地址 --> 物理地址` 之间转化的地址数量，比如逻辑地址 0-170，对应到物理地址是 100-270，那重定位寄存器里存放的就是 100

## 工业革命，分页式管理

分段管理看起来是很容易的，但是存在一个问题，即每个程序的偏移量都是不同的，所以分块的大小也是不相同的，这样不但管理不统一，而且还是有可能产生外碎片。因此，在分段管理的基础上，又出现了分页管理。

分页管理和分段管理的核心一样，都是通过表进行地址映射，不同点在于，分页管理是`将内存拆分成大量大小完全相同的页`，如 2k, 4k 等，这些块在逻辑地址中被称为页，在物理地址中被称为`页帧`。然后依然通过类似的页表，将分页号 对应 查出帧号，然后再通过偏移量即可得到最终的物理地址。

而与分段直接计算不同的是，分页管理中，每个页面大小不是 1 而是 1k 或者 2k 等是 2 的整数次幂，因此分页的物理地址算法为：

```shell
2^S * f + o

# 其中：
# f 是帧号，共有 2^F 个帧
# o 是帧内偏移量，S 位，每帧有 2^S 个字节
# 页号计算：逻辑地址 / 页面大小（取整），比如逻辑地址为 80，页面大小为 50，则其页号就是 1
# 偏移量计算：逻辑地址 % 页面大小，同上页面偏移量是 30
```

由于计算机是二进制的，所以操作系统为了方便计算页号以及偏移量，会把页面大小设置为 2 的整数次幂，比如 2^12=4K，则会有以下例子：

32 位操作系统中，0 号页的地址空间应该是从 0-4095，换算为 2 进制为：`00000000000000000000 000000000000` - `00000000000000000000 111111111111`，同理，1 号页地址空间时 4096-8191，对应的二进制为 `00000000000000000001 000000000000` = `00000000000000000001 111111111111`，可以看出，前 20 位正好代表页号，后 12 位正好代表业内偏移量。

## 近代，离散管理的优化

### TLB

可以知道，不管是分段还是分页管理，在 CPU 寻址时其实都需要两次访存，第一次访问段/页表，第二次才是根据表项计算出结果。为了优化这一点，又出现了快表技术。

快表 TLB，是 MMU 内存管理中的一个硬件寄存器，更靠近 CPU，因此访问速度比内存更快。其在内部`缓存`了分页映射表数据，根据`局部性原理`，加速分页映射查询。

这里引出了**局部性原理**的概念，局部性原理可以说是内存管理的核心原理，其主要分为时间局部性和空间局部性，分别表示：

- 时间局部性：程序在运行时，最近刚刚被引用过的一个内存位置容易再次被引用，比如在调取一个函数的时候，前不久才调取过的本地参数容易再度被调取使用
- 空间局部性：最近引用过的内存位置以及其周边的内存位置容易再次被使用，比如循环

由局部性原理可以知道，程序的运行不论时间还是空间，都是在一个局部范围内的，以此为前提，TLB 的命中率可以达到 90%。

假如访问一次快表耗时 1us，访问以此内存耗时 100us，快表命中率为 90%，那么访问一个逻辑地址的平均耗时为：

```shell
(1 + 100) * 0.9 + (1 + 100 + 100) * 0.1 = 111us
```

可以看到，有了快表技术访问一次逻辑地址的时间几乎可以缩短一倍。

### 多级页表

分页技术虽然可以很好的解决内存碎片问题，但是还是有一个比较大的缺陷：太消耗内存了。如果以一个 64 位系统为例，它的寻址位有 2^64 个，而如果一个页大小为 2^10 的话，那么光分页表就需要 2^64 / 2^10 = 2^54，这个量是十分巨大的。并且，应用程序之间是相互隔离的，多个应用会产生多个副本，进而再次加大了内存消耗。

为了解决这个问题，在分页的基础上又延伸出了多级页表。根据局部性原理我们可以知道，一个程序在一段时间内可能只需要几个页的内存，为此没有必要把所有的逻辑页都加载进内存，而多级页表就能很好的解决这个问题。

其原理是可以把帧号进一步拆分，以一级表帧号+二级表帧组成，然后依次类推，在二级表上也能拆分，最终可以形成一棵树类型的多几分页表。

![multi-page](https://image.dunbreak.cn/os-memory/multi-page.png)

这样做的好处是：

1. 压缩了内存
2. 可以省略那些不存在的分页表

因为，在次级页表中可以通过判断，如果对应的页表不在内存中那可以直接不生成对应的页表。

除此之外，还有一个技术是反向页表，反向页表是对正向页表的逆置。正常的页表是逻辑地址 --> 物理地址。而反向页表是 帧号 --> 分页编号。这样做的目的是为了把分页映射表从内存中移到硬盘中，从而节省空间，但是带来的问题是，操作系统并不知道那些帧号对应那些应用，因此就还需要继续加入缓存，一般采用哈希表。这种技术只在部分大型机中才会使用，目前我们应该接触不到。

## 现代，虚拟内存技术

我之前一直存在一个比较疑惑的问题：为什么 8G 内存的电脑能运行将近 100G 的魔兽世界？这个问题的答案就是虚拟内存技术。

随着行业不断发展，以游戏为例，最早的游戏可能只有几十 K 字节，而之后的游戏可以上百 M 上 G，而内存却远远没有那么大。所以操作系统就把硬盘当做是一个备用的内存，将那些不常用的东西以虚拟内存的概念存放在硬盘中，需要使用时再调用出来。

### 虚拟内存之前

在讲虚拟内存技术之前，需要先了解两个虚拟内存技术出现之前的技术。

#### 覆盖技术

覆盖技术的核心是以大程序不是同时运行而是有时间先后的概念的，这样就可以做到，把已经运行得到结果的程序从内存中清除，然后把后续需要执行的程序覆盖到刚才的位置，比如：

```shell
                          A                              -->                             常驻内存
                      B        C                         -->                           统一覆盖区BC
                   D     E   F   G                       -->            统一覆盖区DE         和           统一覆盖区FG
```

一个大程序可能是由上述 ABCDEFG7 个函数组成的，其中 A 是主函数，内部调用了 B 和 C，那么就可以把 B 和放在同一个覆盖区内，B 又调了 DE，那么 DE 就在一个覆盖区呢，同理 FG 也是一个。这样，一个程序所需的内存就被进行了压缩。A 程序常驻内存，BC 的内存占比就成了 B 和 C 最大的那个，其他的覆盖区也一样。在 B 程序运行结束后，把 C 需要的内存覆盖到 B 对应的覆盖区内。从而达到整体大程序可以执行的目的。

举例：Turbo Pascal 就是有处理这个能力的 runtime。

#### 交换技术

类同与覆盖技术，交换技术不是将内存覆盖，而是将还没有执行的程序放在硬盘中，当程序加载时把已经结束的程序和其进行交换。可以看到交换技术已经有了虚拟内存的影子。

交换技术又两个重要的点：

1. 交换的时机，因为交换是将内存中的数据库和硬盘中的数据进行交换，需要 IO，并且内存的速度和硬盘的速度是差几个数量级的，如果频繁交换会存在非常大的性能问题
2. 如果一个程序交换出去之后又被调用到，之后会被再交换回来，那它对应的物理地址和逻辑地址的映射如何实现，很有可能第二次进入之后的映射已经和第一次不一样了，所以这也需要自己考虑

### 虚拟内存

有上述的概念可知，交换技术和覆盖技术都存在问题，所以虚拟内存技术才被引入。

虚拟内存的核心其实就是上述两个技术的配合，其原理是将内存分页之后按页进行覆盖和交换。在内存还有空间时，操作系统将硬盘虚拟内存中的数据载入内存；在内存已经满了之后，操作系统将暂时不会用或者已经很久没用的内存和需要加载的硬盘程序进行交换。

当程序运行时，如果需要访问不在内存中的虚拟内存内容，程序会触发一个缺页异常，从而引发一个缺页中断，在这次缺页中断中，操作系统就会将虚拟内存中的内容加载进内存中，如果内存已经满载了，则需要将内存中不会使用的内容与之进行交换。

从而又可以知道，一段程序的写法是会影响操作系统触发中断的频率的，比如：

当前操作系统中的分页大小是 4K，有如下程序的两种不同的写法：

```c
/*写法一*/
int[] array = new int[1024][1024];
for (int j = 0; j < 1024; j++)
{
    for (int i = 0; i < 1024; i++)
    {
        array[i][j] = i;
    }
}

/*写法二*/
int[] array = new int[1024][1024];
for (int i = 0; i < 1024; i++)
{
    for (int j = 0; j < 1024; j++)
    {
        array[i][j] = j;
    }
}
```

我们知道一个 int 在 32 位操作系统占 4 个字节，1024 个 int 正好是 4K，为一个分页的大小。且 C 语言对待二维数组是行优先的，也就是优先写入行。

在写法一中，数组 array 是逐行创建的，而数组大小在创建时会预留 1024 个，所以就导致写法一中触发了 1024 \* 1024 个缺页中断，每次中断都有可能触发系统 IO，性能很差。

而写法二中，array 是按照 C 语言标准逐行写入的，在每一行写入时并不会触发缺页中断，只有在一行写完之后才会触发一个，最终只会触发 1024 个缺页中断。由此可知，写法二的性能是远优于写法一的。

为了更好的配合虚拟内存使用，在页表中还有几个没有说明的标记位，以下是一个分页映射表中的一行内容：

```shell
0xABCDE... 1011 DEFABC...
```

其中有 4 位预留 1011 用于和虚拟内存技术进行配合使用。其中：

1. 第一位（最右边的 1）是驻留位：表示之前提到的是否存在当时没有说明，现在我们知道了，如果那个标记为标明存在，说明那段逻辑内存已经从虚拟内存中加载进入了物理内存
2. 第二位是保护位：用来存储当前内存的可访问状态，比如可读可写，只读只写
3. 第三位是修改位：用来存储当前的内存是否在内存中被修改，如果被修改了，那么当这段内存被交换回虚拟内存时，需要在硬盘中进行覆盖
4. 第四位是访问位：用来标记这段内存是否在内存中被访问过，如果被访问过则标记它，用于分页置换算法

### 页面置换算法

伴随着虚拟内存技术的出现，当内存足够时我们只需要考虑将逻辑页加载进入物理内存即可。但是如果物理内存已经满了，就需要用到上述讲到的类似于交换的功能。

而触发一次缺页中断将逻辑内存加载进入物理内存的时间消耗大概只有 10ns，而进行页面置换操作需要读盘，这个操作可能就是 5ms，可以看到其中差 4-5 个数量级。

因此诞生出几个页面置换算法用以优化页面交换的性能以及利用率。

为了便于理解分页的开销，使用有效的存储器访问时间这个概念可以当做是一次缺页中断所需要的开销时间，即：

> effective memory access time （EAT）= 访问时间 _ 页面命中几率 + page fault 处理时间 _ page fault 几率

可以举一个例子：
访问时间： 10ns
磁盘访问时间：5ms
p = page fault 几率 = 缺页几率
q = dirty page 几率 = 对内存进行写操作导致内存外存数据不一致带来的写盘几率
可得： EAT = 10*(1-p) + 5000000*p\*(1+q)

通过上面的例子可以知道，q 在操作系统运行过程中是随机产生的，我们无法控制，而内存访问时间和读盘时间取决于硬件也无法控制，所以，如果要提升页面置换的性能，唯一的方法就是`降低 page fault 几率`。

#### 最优页面置换算法

所谓最优页面置换算法就是系统最想达到的置换算法，其逻辑是看未来那些内存不需要使用，从而优先把那些不需要使用的内存先换出，这样就可以降低缺页异常发生的概率。举一个例子：如果有一个读入序列为： c a d b e b a b c d ，且此时的物理内存大小是 4，且已经存在了 a b c d 四个页面。

则：刚开始的 1-4 物理内存命中，而到 5 访问到 e 时，由于内存满了需要置换，如果使用最优页面算法，则需要向后看未来的内存使用情况，可知未来最不可能使用到的内存是 d，所以如果是最优算法，则会把 d 置换出去，此时，内存中为 abce，当最后访问到 e 时，则会再出发置换逻辑将 a 置换出去，可以看到整个过程只会触发两次置换操作。

![page-changing-perfect](https://image.dunbreak.cn/os-memory/page-changing-perfect.png)

综上可知，由于内存操作都是事件性质的无法预估未来，所以最优页面置换算法只是一种最优解，只能作为标杆让后续的算法尽可能地达到其置换效果。

#### 先进先出算法

先进先出算法非常简单，就是使用队列管理内存，首先读入的首先换出。还是以上面的那个例子为例，当系统第一次访问 5e 时，第一个被换出的就是 a，同理依次换出 b,c,d,e。可知一共触发了 5 次置换操作，其性能非常低，几乎不会使用。并且，先进先出算法会存在 `Belady 现象`，不是一个栈算法，所以基本只作为参考，不能使用。

![page-changing-fifo](https://image.dunbreak.cn/os-memory/page-changing-fifo.png)

从上图可知，FIFO 算法需要 5 次缺页中断，其性能是很低的。

#### LRU 算法

之前学数据结构的使用做缓存就使用到了 LRU cache，此次这个算法逻辑和 LRU cache 完全一致，将最近最少使用的内存进行置换，这种算法可以看做是最优置换算法的反制，最优算法看未来，此算法看过去。同样以上述的例子为例，如果使用 LRU 算法，则最终的置换操作为：

![page-changing-lru](https://image.dunbreak.cn/os-memory/page-changing-lru.png)

可以看到，此算法只用触发 3 页面置换，几乎接近了最优置换算法。但是，此算法**实现起来非常复杂**，不论是使用栈还是链表，都需要遍历整个内存才能找出需要置换出的内存页，虽然页面置换算法高效，但是实现的复杂度和性能却非常高，完全是舍本逐末，所以，此算法也不会使用在操作系统中。

#### 时钟页面置换算法

为了更好地解决复杂度和性能的平衡点，时钟置换算法出现了。

Clock 页面算法和 LRU 近似，可以看做是对 FIFO 的一种改进，其逻辑是，将内存管理为一个环形队列，有一个和时钟一样的指针 P 指向环形队列中的一个内存进行管理。

![clock-algorithm](https://image.dunbreak.cn/os-memory/clock-algorithm.png)

这里需要使用之前讲过的页表中的标记为中的`访问位`。

这个访问位是当该内存被访问过之后，由`硬件自动标记`为 1 的。因此，每当内存被访问时，指针就会向下移动到下一个页面，看此页面的访问位是否为 1，如果是 1，则将其置为 0 继续想下访问；如果是 0，则将其换出。

可以看到这个访问位由硬件标记，由操作系统置 0，可以当做是一种简单的缓存逻辑，多一条命。

还是以上述的例子为例，如果使用 Clock 置换算法，则结果为：

![page-changing-clock](https://image.dunbreak.cn/os-memory/page-changing-clock.png)

可以看到性能已经有所提升了，当后续的序列更多是，此算法的性能会更好，由大量的测算可得此算法的性能几乎和 LRU 相同。

#### 二次机会法（Enhanced Clock）

此算法可以看做是 Clock 算法的一种改进。

在 Clock 算法中，只是用到了页面的访问位进行判断，但是一个内存的访问状况其实`分读和写`两种，因此，应该加入 `dirty 位`把两个位共同进行判断。

因为从之前的概念我们可以知道，如果一个内存页被 dirty，那么如果该内存被置换则需要把此内存回写到虚拟内存中，这也是一笔开销。因此，我们应该`尽量减少被 dirty 的内存被置换`。

二次机会法就是这样一种算法， 其将可访问位和 dirty 位共同组成判断条件（优先修改可访问位，即如果标记为为 01，则可访问位是 0，dirty 位是 1），如果指针知道的内存为 11，则会把其置为 01 向下移动；如果内存标记位为 01，则改为 00 继续向下移动；直到碰到 00 的标记为，才会将其置换出去。也就是给了一个内存两条命。

同样以最开始的例子为例，如果使用二次机会法，则置换逻辑为：

![page-changing-enhanced-clock](https://image.dunbreak.cn/os-memory/page-changing-enhanced-clock.png)

> 其中 a^w 表示是对 a 的写操作。

综上：优先置换只读页，减少硬盘读写次数。

#### 最不常用算法（Least Frequently Used）

意思是将内存中使用频率最低的页面置换出去。其类似于 LRU 不过 LRU 是以使用时间的历史时间进行换入换出，而 LFU 则是以使用频率进行的。

因此，为了达到计数的目的，需要对每个页面设置一个访问计数器，没当页面被访问时，对应的计数器+1，在发生缺页中断时，淘汰数值最小的那个页面。

### Belady 现象

Belady 不是一个单词，而是一个叫 Belady 的计算机科学家发现的一种页面置换算法的现象，说的是当`页帧增大`时，缺页中断的次数`没有减少反而增加`了。

比较典型的算法就是 FIFO 算法，如下图所示：

![belady-01](https://image.dunbreak.cn/os-memory/belady-01.png)

从图中我们可以看到，FIFO 算法在`页帧为 3` 时，发生了 `9 次`缺页中断。

而如果将页帧增加到 4，结果如下图所示：

![belady-02](https://image.dunbreak.cn/os-memory/belady-02.png)

可以看到，当`页帧数为 4` 时，缺页中断反而增加到了 `10 次`。

而如果使用 LRU 算法，如下图：

![belady-03](https://image.dunbreak.cn/os-memory/belady-03.png)

可以看到，LRU 算法在帧数从 3 增加到 4 时，缺页次数从 10 次降到了 8 次。

这就是 Belady 现象，出现这种现象的根本原因是，相应的算法比如 FIFO 算法不满足栈算法原理。

### 全局页面置换算法

以上讲到的所有算法，都被称为`局部页面替换算法`，因为这些算法都只考虑了一个不会改变进程的情况。但是，由`局部性原理`我们可以知道，一个程序在开始启动，到运行，到最终结束，整个过程的每个时间段所需要的内存其实都是不同的，然而上述的算法都不满足这种内存大小动态变化的情况，为此引入了`工作集`的概念。

**工作集**：一个进程当前正在使用的`逻辑页面的集合`。一般用函数 W(t, δ)表示。其中，t 是当前的执行时刻，δ 是工作集窗口，即一个定长的页面访问的时间窗口。

由此可以知道，工作集是随着时间随时改变的，时刻 1 可能是 3 个页帧，时刻 2 可能是 2 个，时刻 3 可能是 5 个。其原理就是算法中的`滑动窗口`，当 δ 固定时，随着时间 t 的改变，所需的 W 值是不断改变的。

与工作集对应的一个概念是**常驻集**，常驻集是指在当前时刻，进程`实际驻留在内存中的页面集合`。

工作集和常驻集这两个概念的引入，主要是为了解决动态进程运行过程中，每个时刻所需的内存都是不相同的问题。

当工作集和常驻集差距很大时，进程会多次触发缺页异常，导致性能问题，因此，我们应该尽可能的让常驻集与工作集重合，且不能让常驻集非常大，因为操作系统要尽量空出内存提供给其他需要的进程使用。所以保证工作集和常驻集的动态平衡，让缺页异常保持在一个可控的范围内，从而达到整个操作系统的所有进程的总体缺页率下降，才是真正想要的页面置换算法，即`全局页面置换算法`。

#### 工作集页面置换算法

这是两个全局置换算法中的一个，主要逻辑是`固定滑动窗口的大小`，比如 τ=4，在这个时间段内，存在的内存被保留，否则就被置换出去。如下图所示：

![work-set](https://image.dunbreak.cn/os-memory/page-changing-work-set.png)

从上图可以看到，在时刻 2 page e 被置换出去了，原因是 page e 是在 t = -2 被加入内存的，而 τ=4，而在时刻 2 page e 已经过期，所以就算没有超过页帧数，对应的 page 也会被置换。对应的在时刻 4,7,8 都触发了页面置换逻辑。

#### 缺页率页面置换算法

> 缺页率 = 缺页次数 / 内存访问次数

因此该算法说的是常驻集可以动态改变，在进程初始化时可能会分配一个内存，随着进程的运行过程，根据缺页率算法会动态修改常驻集的大小，从而达到全局页面置换的功能。

此算法的逻辑是：记录两次缺页中断的时间，并且记录阈值即滑动窗口大小，判断差值和阈值，如果差值大于阈值，则将在这段时间没有访问到的页面置换出内存，反之，则只将缺页加入内存。

可以记录上一次缺页中断的时间点 tlast，以及本地的缺页中断短时间点 tcurrent，可以动态设置时间 T 为滑动窗口的大小，则有以下逻辑：

1. 如果 T < tcurrent - tlast，则说明当前常驻集过多，需要释放一些内存供其他进程使用，会将这段时间内没有访问到的页面置换出内存
2. 如果 T >= tcurrent - tlast，则说明现在的缺页频率过于频繁不应该再置换页面，只是将缺页加入内存，再不做其他事

如下图所示，T = 2：

![page-fault](https://image.dunbreak.cn/os-memory/page-changing-page-fault.png)

从上图可以看到如下的调用过程：

1. 初始时，内存中有 a, d, e 三个页
2. 时刻 1 时，发生缺页中断，将 c 置换如内存，并且记录 tlast = 1 3.时刻 4，发生缺页中断，将 b 置换入内存，并且记录 tcurrent = 4，此时 tcurrent - tlast = 3，满足 T < tcurrent - tlast，则需要将这段时间内没有访问到的其他页置换出去，则将 a,e 置换出内存
3. 时刻 6，将缺页加入内存，并且差值<=T，则什么事都不做
4. 时刻 6，将缺页加入内存，并且差值<=T，则什么事都不做
5. 时刻 9，将缺页 a 加入内存，并且此时差值=3>T，则将 6~9 之间没有访问到的内存 b,d 置换出内存
6. 时刻 10，加入缺页，并且差值小于 T，不做任何操作

由此可以看到，这种算法中，常驻集是动态变化的，这样做，即可以降低缺页率，又可以保证有足够多的内存提供给其他进程使用。

### 抖动问题

抖动问题其实说的是，如果一个进程的常驻集远小于工作集，那说明系统会将产生大量的缺页中断，操作系统需要频繁地进行页面置换操作，从而使得进程的运行速度变慢。一般把这种状态称为抖动。

由此可见，尽可能地使常驻集和工作集相交是解决此问题最好的方法，这个问题的本质，其实就是上述各种全局页面置换算法中，滑动窗口大小如何合理定义的问题。应该尽可能的定义一个有效的滑动窗口，使得缺页异常和常驻集大小保持在一个动态平衡的状态。

这个问题由以下一个坐标图展示：

![thrashing](https://image.dunbreak.cn/os-memory/thrashing.png)

其中左纵轴是 CPU 利用率（不是 CPU 使用率），右纵轴是平均缺页时间与缺页服务时间（图中翻译写错了）的比值，横轴 N 是系统中并发执行进程的个数。

可知，当比值为 1 时，即为 CPU 被程序最佳利用的点，并且这个点与 CPU 利用率相交的那个位置，即为 CPU 动态平衡进程个数的点。

## 记忆链

1. 最开始的操作系统是将连续的内存直接装入内存的
2. 这时发现频繁装入装出会产生大量外碎片，导致可用的连续内存减少从而导致虽然内存总空间足够但是连续空间不够的情况
3. 为了解决这个问题出现了离散分配方式，首先是分段技术。分段技术是将进程拆分成一个一个的功能段，操作系统将这些段分别映射到内存中，并且功能段之间可以不连续并且可以离散
4. 但是依然存在外碎片的问题以及分段大小不同导致的偏移量不同无法统一管理，为此出现了分页技术。分页技术本质和分段技术相同，但是将分段更进一步将内存拆分成一个大小相同比如 4K 的页，对这些页进行统一管理。这样外碎片就算产生由于页很小所以不会造成过多浪费
5. 但是分页技术也有一个弊端，即由于页很小，导致页表非常大的问题。在此基础上出现了快表 TLB 和多级页表技术
6. 分页技术之后，根据局部性原理，人们发现程序运行过程中仅有部分内存被使用，其余的都访问不到，因此出现了虚拟内存技术。即将整个硬盘作为逻辑内存地址，程序在开始会被分配逻辑内存，当相应的代码被访问到，才会将逻辑内存页置换到物理内存中
7. 为此出现了页面置换算法，主要有：最优置换、先进先出、LRU、时钟、二次机会以及最不常用算法用以解决缺页中断率的问题
8. 但是上述算法值考虑到静态的情况，但是程序运行时很可能是刚开始需要的内存多，中间需要的内存少，最后需要的内存又多这种情况，为了模拟动态内存分配，出现了工作集和常驻集的概念
9. 以工作集合常驻集为基础，出现了基于滑动窗口为基础的两个全局页面置换算法：工作集算法和缺页率算法。

以上就是操作系统内存管理模块的主要功能了，其实还有段页式存储我这里都没有写。可以看到当从头到尾学完之后，开篇问的那几个问题都很幼稚，简单的内存调用需要这么多复杂的算法支持着。
